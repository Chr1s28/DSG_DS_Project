[
  {
    "objectID": "3_modellierungsbericht.html",
    "href": "3_modellierungsbericht.html",
    "title": "Modellierungsbericht",
    "section": "",
    "text": "Das Ziel dieses Projekts ist es, ein Regressionsmodell zu trainieren, welches die Popularität (popularity) eines Songs basierend auf seinen intrinsischen Eigenschaften vorhersagen kann. Zu diesen Eigenschaften gehören Audio-Features wie Tanzbarkeit, Energie oder Tempo, sowie später semantische Merkmale aus den Songtexten.\nIm Verlauf der Modellierung haben wir drei verschiedene Modellarchitekturen evaluiert. Zunächst starteten wir mit einem Basismodell (Linear Regression) und einem fortgeschrittenen Modell (Random Forest) auf dem bereinigten Spotify-Datensatz (Spotify_Cleaned_No_Noise). Für die finale Architektur (XGBoost) erweiterten wir zusätzlich den Datensatz um die Songtext-Embeddings (Spotify_Genius_No_Noise_Embeddings), um zu prüfen, ob die semantische Analyse der Lyrics die Vorhersagegenauigkeit verbessern kann."
  },
  {
    "objectID": "3_modellierungsbericht.html#modellbeschreibung",
    "href": "3_modellierungsbericht.html#modellbeschreibung",
    "title": "Modellierungsbericht",
    "section": "Modellbeschreibung",
    "text": "Modellbeschreibung\nUm eine Baseline für unsere Vorhersagen zu etablieren, verwenden wir ein einfaches lineares Regressionsmodell (LinearRegression aus sklearn). Dieses Modell versucht, eine lineare Beziehung zwischen den unabhängigen Variablen (Audio-Features) und der abhängigen Variable (popularity) zu modellieren.\nDa wir im Datenbericht gesehen haben, dass die Korrelationen zwischen einzelnen Audio-Features und der Popularität eher schwach sind, erwarten wir von diesem einfachen Modell keine extrem hohe Genauigkeit. Es dient jedoch als wichtiger Referenzpunkt, um die Leistungssteigerung komplexerer Modelle beurteilen zu können.\n\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import train_test_split\n\ndf_ml = pd.read_csv(Path(\"../data/cleaned_no_noise_tracks_features.csv\"), keep_default_na=False, na_values=[\"\"])\n# Drop non-feature columns\ndf_ml = df_ml.drop(\n    columns=[\"id\", \"name\", \"album\", \"album_id\", \"artists\", \"artist_ids\", \"track_number\", \"disc_number\", \"year\"]\n)\n\n\n# 1. Define X (Features) and y (Target)\nX = df_ml.drop(columns=[\"popularity\"])\ny = df_ml[\"popularity\"]\n\n# 2. Split: 80% for Training, 20% for Testing\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=67)\n\nprint(f\"Training Data Shape: {X_train.shape}\")\nprint(f\"Testing Data Shape: {X_test.shape}\")\n\nTraining Data Shape: (377182, 15)\nTesting Data Shape: (94296, 15)\n\n\n\nlr_model = LinearRegression()\nlr_model.fit(X_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegression?Documentation for LinearRegressioniFitted\n        \n            \n                Parameters\n                \n\n\n\n\nfit_intercept \nTrue\n\n\n\ncopy_X \nTrue\n\n\n\ntol \n1e-06\n\n\n\nn_jobs \nNone\n\n\n\npositive \nFalse"
  },
  {
    "objectID": "3_modellierungsbericht.html#resultate",
    "href": "3_modellierungsbericht.html#resultate",
    "title": "Modellierungsbericht",
    "section": "Resultate",
    "text": "Resultate\nWie erwartet liefert das lineare Basismodell eher schwache Ergebnisse. Mit einem \\(R^2\\)-Wert von lediglich 0.054 kann das Modell nur etwa 5.4% der Varianz in der Popularität erklären. Der Root Mean Squared Error (RMSE) von 12.92 bedeutet, dass die Vorhersagen im Durchschnitt um fast 13 Punkte vom tatsächlichen Wert abweichen.\nDieses Ergebnis bestätigt, dass die Zusammenhänge zwischen Audio-Features und Popularität nicht linearer Natur sind. Um komplexere, nicht-lineare Muster zu erfassen, werden wir im nächsten Schritt auf entscheidungsbaumbasierte Modelle wie Random Forest wechseln.\n\ny_pred_lr = lr_model.predict(X_test)\nrmse_lr = np.sqrt(mean_squared_error(y_test, y_pred_lr))\nr2_lr = r2_score(y_test, y_pred_lr)\n\nprint(f\"Baseline RMSE: {rmse_lr:.2f}\")\nprint(f\"Baseline R^2: {r2_lr:.4f}\")\n\nBaseline RMSE: 12.92\nBaseline R^2: 0.0540"
  },
  {
    "objectID": "3_modellierungsbericht.html#modellinterpretation",
    "href": "3_modellierungsbericht.html#modellinterpretation",
    "title": "Modellierungsbericht",
    "section": "Modellinterpretation",
    "text": "Modellinterpretation\nTrotz der geringen Vorhersagekraft (\\(R^2 \\approx 0.05\\)) geben die Koeffizienten Aufschluss über lineare Tendenzen. Sie zeigen Richtung und Stärke des Einflusses einzelner Features auf die Popularität, wobei aufgrund unterschiedlicher Skalen (z.B. dB vs. 0-1) Vorsicht beim direkten Vergleich geboten ist.\n\n\nCode\nimport matplotlib.pyplot as plt\n\ncoef_df = pd.DataFrame({\"Feature\": X.columns, \"Coefficient\": lr_model.coef_})\ncoef_df[\"Abs_Coefficient\"] = coef_df[\"Coefficient\"].abs()\ncoef_df = coef_df.sort_values(\"Abs_Coefficient\", ascending=False)\n\nplt.figure(figsize=(10, 6))\nplt.barh(coef_df[\"Feature\"].head(10), coef_df[\"Coefficient\"].head(10))\nplt.xlabel(\"Koeffizient\")\nplt.title(\"Top 10 Einflussfaktoren (Linear Regression)\")\nplt.gca().invert_yaxis()\nplt.grid(axis=\"x\", linestyle=\"--\", alpha=0.7)\nplt.show()"
  },
  {
    "objectID": "3_modellierungsbericht.html#modellbeschreibung-1",
    "href": "3_modellierungsbericht.html#modellbeschreibung-1",
    "title": "Modellierungsbericht",
    "section": "Modellbeschreibung",
    "text": "Modellbeschreibung\nDa das lineare Modell nur schwache Ergebnisse lieferte, wechseln wir zu einem Random Forest Regressor. Entscheidungsbaum-basierte Modelle wie Random Forest sind deutlich besser geeignet, um nicht-lineare Zusammenhänge und Interaktionen zwischen Features zu erfassen (z.B. könnte eine hohe energy nur in Kombination mit hoher danceability zu Popularität führen).\nZudem ist Random Forest robust gegenüber Ausreissern und erfordert keine Skalierung der Daten, was die Handhabung der unterschiedlichen Wertebereiche (z.B. loudness in dB vs. valence 0-1) vereinfacht.\n\nfrom sklearn.ensemble import RandomForestRegressor\n\ndf_ml = pd.read_csv(Path(\"../data/cleaned_no_noise_tracks_features.csv\"), keep_default_na=False, na_values=[\"\"])\ndf_ml = df_ml.drop(\n    columns=[\"id\", \"name\", \"album\", \"album_id\", \"artists\", \"artist_ids\", \"track_number\", \"disc_number\", \"year\"]\n)\n\n# 1. Define X (Features) and y (Target)\nX = df_ml.drop(columns=[\"popularity\"])\ny = df_ml[\"popularity\"]\n\n# 2. Split: 80% for Training, 20% for Testing\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=67)\n\nprint(f\"Training Data Shape: {X_train.shape}\")\nprint(f\"Testing Data Shape: {X_test.shape}\")\n\nTraining Data Shape: (377182, 15)\nTesting Data Shape: (94296, 15)\n\n\n\nrf_model = RandomForestRegressor(n_estimators=100, random_state=67, n_jobs=-1)\nrf_model.fit(X_train, y_train)\n\nRandomForestRegressor(n_jobs=-1, random_state=67)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressor?Documentation for RandomForestRegressoriFitted\n        \n            \n                Parameters\n                \n\n\n\n\nn_estimators \n100\n\n\n\ncriterion \n'squared_error'\n\n\n\nmax_depth \nNone\n\n\n\nmin_samples_split \n2\n\n\n\nmin_samples_leaf \n1\n\n\n\nmin_weight_fraction_leaf \n0.0\n\n\n\nmax_features \n1.0\n\n\n\nmax_leaf_nodes \nNone\n\n\n\nmin_impurity_decrease \n0.0\n\n\n\nbootstrap \nTrue\n\n\n\noob_score \nFalse\n\n\n\nn_jobs \n-1\n\n\n\nrandom_state \n67\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nccp_alpha \n0.0\n\n\n\nmax_samples \nNone\n\n\n\nmonotonic_cst \nNone\n\n\n\n\n            \n        \n    \n\n\nWir verwenden einen Random Forest mit 100 Entscheidungsbäumen (n_estimators=100). Dieser Wert bietet einen guten Kompromiss zwischen Rechenzeit und der Reduktion der Varianz (Overfitting-Vermeidung). Der random_state wurde auf 67 fixiert, um die Ergebnisse reproduzierbar zu halten. Alle anderen Hyperparameter wurden auf den Standardwerten von sklearn belassen, da diese für eine erste Evaluation meist robuste Ergebnisse liefern."
  },
  {
    "objectID": "3_modellierungsbericht.html#resultate-1",
    "href": "3_modellierungsbericht.html#resultate-1",
    "title": "Modellierungsbericht",
    "section": "Resultate",
    "text": "Resultate\nDer Wechsel auf Random Forest brachte eine deutliche Verbesserung der Modellgüte. Der \\(R^2\\)-Wert stieg auf 0.173, was bedeutet, dass wir nun rund 17.3% der Varianz erklären können, mehr als das Dreifache im Vergleich zur linearen Regression (\\(R^2 \\approx 0.054\\)). Auch der RMSE verbesserte sich leicht auf 12.08.\n\ny_pred_rf = rf_model.predict(X_test)\nrmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\nr2_rf = r2_score(y_test, y_pred_rf)\n\nprint(f\"Random Forest RMSE: {rmse_rf:.2f}\")\nprint(f\"Random Forest R^2: {r2_rf:.4f}\")\n\nRandom Forest RMSE: 12.08\nRandom Forest R^2: 0.1727"
  },
  {
    "objectID": "3_modellierungsbericht.html#modellinterpretation-1",
    "href": "3_modellierungsbericht.html#modellinterpretation-1",
    "title": "Modellierungsbericht",
    "section": "Modellinterpretation",
    "text": "Modellinterpretation\nDie Feature Importance Grafik verdeutlicht, welche Merkmale der Random Forest am häufigsten für Entscheidungen (Splits) in den Bäumen verwendet hat:\n\ntrack_age_days: Mit Abstand am wichtigsten ist das Veröffentlichungsdatum. Dies bestätigt unsere Intuition dass das Alter eines Songs auf Spotify ein Haupttreiber für Popularität ist.\nLoudness War: loudness ist das wichtigste Audio-Feature. Dies korrespondiert mit der im Datenbericht gezeigten Korrelation, lautere Tracks setzen sich in Playlists eher durch.\nAkustische Merkmale: Features wie acousticness, duration_ms und energy spielen eine moderate Rolle.\nIrrelevanz von Keys: Technische Merkmale wie key (Tonart) oder mode (Dur/Moll) haben fast keinen Einfluss auf die Vorhersage.\n\n\n\nCode\nimport pandas as pd\nimport plotly.express as px\nimport plotly.io as pio\n\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\n\nimportances = rf_model.feature_importances_\nfeature_names = X_train.columns\n\ndf_imp = pd.DataFrame({\"Feature\": feature_names, \"Importance\": importances}).sort_values(\n    by=\"Importance\", ascending=True\n)\n\nfig = px.bar(\n    df_imp.tail(15),\n    x=\"Importance\",\n    y=\"Feature\",\n    orientation=\"h\",\n    title=\"Random Forest: Feature Importance (Top 15)\",\n    labels={\"Importance\": \"Relative Wichtigkeit\"},\n    text_auto=\".3f\",\n)\n\nfig.update_layout(height=600, yaxis_title=None)\n\nfig.show()"
  },
  {
    "objectID": "3_modellierungsbericht.html#spotify_cleaned_no_noise",
    "href": "3_modellierungsbericht.html#spotify_cleaned_no_noise",
    "title": "Modellierungsbericht",
    "section": "Spotify_Cleaned_No_Noise",
    "text": "Spotify_Cleaned_No_Noise\n\nModellbeschreibung\nFür den finalen Modellierungsansatz wechseln wir vom Random Forest auf den XGBoost Regressor (Extreme Gradient Boosting). Während Random Forest auf “Bagging” (paralleles Trainieren) setzt, nutzt XGBoost das “Gradient Boosting”-Verfahren, bei dem Modelle sequenziell trainiert werden, um die Fehler der Vorgänger zu korrigieren. Dies führt oft zu einer höheren Präzision bei komplexen Datenstrukturen.\nDazu verwenden wir das xgboost python package und wie bisher wieder das Spotify_Cleaned_No_Noise Dataset. Mehr Informationen zu XGBoost findet man auf der offiziellen Dokumentation.\n\nfrom xgboost import XGBRegressor\n\ndf_ml = pd.read_csv(Path(\"../data/cleaned_no_noise_tracks_features.csv\"), keep_default_na=False, na_values=[\"\"])\ndf_ml = df_ml.drop(\n    columns=[\"id\", \"name\", \"album\", \"album_id\", \"artists\", \"artist_ids\", \"track_number\", \"disc_number\", \"year\"]\n)\n\nX = df_ml.drop(columns=[\"popularity\"])\ny = df_ml[\"popularity\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=67)\n\nxgb_model = XGBRegressor(n_estimators=2000, learning_rate=0.1, random_state=67, n_jobs=-1)\nxgb_model.fit(X_train, y_train)\n\nXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             feature_weights=None, gamma=None, grow_policy=None,\n             importance_type=None, interaction_constraints=None,\n             learning_rate=0.1, max_bin=None, max_cat_threshold=None,\n             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n             max_leaves=None, min_child_weight=None, missing=nan,\n             monotone_constraints=None, multi_strategy=None, n_estimators=2000,\n             n_jobs=-1, num_parallel_tree=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBRegressor?Documentation for XGBRegressoriFitted\n        \n            \n                Parameters\n                \n\n\n\n\nobjective \n'reg:squarederror'\n\n\n\nbase_score \nNone\n\n\n\nbooster \nNone\n\n\n\ncallbacks \nNone\n\n\n\ncolsample_bylevel \nNone\n\n\n\ncolsample_bynode \nNone\n\n\n\ncolsample_bytree \nNone\n\n\n\ndevice \nNone\n\n\n\nearly_stopping_rounds \nNone\n\n\n\nenable_categorical \nFalse\n\n\n\neval_metric \nNone\n\n\n\nfeature_types \nNone\n\n\n\nfeature_weights \nNone\n\n\n\ngamma \nNone\n\n\n\ngrow_policy \nNone\n\n\n\nimportance_type \nNone\n\n\n\ninteraction_constraints \nNone\n\n\n\nlearning_rate \n0.1\n\n\n\nmax_bin \nNone\n\n\n\nmax_cat_threshold \nNone\n\n\n\nmax_cat_to_onehot \nNone\n\n\n\nmax_delta_step \nNone\n\n\n\nmax_depth \nNone\n\n\n\nmax_leaves \nNone\n\n\n\nmin_child_weight \nNone\n\n\n\nmissing \nnan\n\n\n\nmonotone_constraints \nNone\n\n\n\nmulti_strategy \nNone\n\n\n\nn_estimators \n2000\n\n\n\nn_jobs \n-1\n\n\n\nnum_parallel_tree \nNone\n\n\n\nrandom_state \n67\n\n\n\nreg_alpha \nNone\n\n\n\nreg_lambda \nNone\n\n\n\nsampling_method \nNone\n\n\n\nscale_pos_weight \nNone\n\n\n\nsubsample \nNone\n\n\n\ntree_method \nNone\n\n\n\nvalidate_parameters \nNone\n\n\n\nverbosity \nNone\n\n\n\n\n            \n        \n    \n\n\nWir haben uns für eine konservative Konfiguration entschieden, um die Generalisierungsfähigkeit zu erhöhen. Die Lernrate wurde auf 0.1 gesetzt (niedriger als der Standard von 0.3), was das Risiko von Overfitting verringert, aber im Gegenzug mehr Iterationen erfordert. Dementsprechend haben wir die Anzahl der Boosting-Runden auf 2000 (n_estimators=2000) festgelegt, um dem Modell genügend Kapazität zu geben, die Fehler der Vorgänger-Bäume schrittweise zu korrigieren.\n\n\nResultate\nDer Einsatz von XGBoost auf dem identischen Dataset brachte keine nennenswerte Verbesserung gegenüber dem Random Forest.\nMit einem R2-Wert von 0.174 und einem RMSE von 12.07 sind die Ergebnisse praktisch identisch:\n\ny_pred_xgb = xgb_model.predict(X_test)\nrmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\nr2_xgb = r2_score(y_test, y_pred_xgb)\n\nprint(f\"XGBoost RMSE: {rmse_xgb:.2f}\")\nprint(f\"XGBoost R^2: {r2_xgb:.4f}\")\n\nXGBoost RMSE: 12.07\nXGBoost R^2: 0.1742\n\n\n\n\nModellinterpretation\nDiese Resultate deuten stark darauf hin, dass der Informationsgehalt der rein akustischen Merkmale ausgeschöpft ist. Selbst ein leistungsstärkerer Algorithmus wie XGBoost kann keine Muster finden, die in den Daten schlicht nicht vorhanden sind. Um die Vorhersagekraft signifikant zu steigern, benötigen wir qualitativ neue Informationen. Daher erweitern wir im nächsten und letzten Schritt den Datensatz um die Songtext-Embeddings.\nBei der Feature Importance Visualisierung sehen wir wieder ähnliche Muster wie beim Random Forest Modell, was diese Vermutung bestätigt:\n\n\nCode\nimport matplotlib.pyplot as plt\nfrom xgboost import plot_importance\n\nplt.figure(figsize=(10, 8))\nplot_importance(xgb_model, max_num_features=20, height=0.5)\nplt.show()\n\n\n&lt;Figure size 1000x800 with 0 Axes&gt;"
  },
  {
    "objectID": "3_modellierungsbericht.html#spotify_genius_no_noise_embeddings",
    "href": "3_modellierungsbericht.html#spotify_genius_no_noise_embeddings",
    "title": "Modellierungsbericht",
    "section": "Spotify_Genius_No_Noise_Embeddings",
    "text": "Spotify_Genius_No_Noise_Embeddings\n\nModellbeschreibung\nUm unsere Hypothese zu prüfen, dass semantische Inhalte der Songtexte die Vorhersage verbessern können, trainieren wir das XGBoost-Modell erneut. Diesmal verwenden wir jedoch den erweiterten Datensatz Spotify_Genius_No_Noise_Embeddings, der zusätzlich zu den Audio-Features auch die Vektor-Repräsentationen der Lyrics enthält.\nDie Embeddings liegen ursprünglich als einzelne Spalte mit Listen von Floats vor. Für das Training müssen diese in separate Feature-Spalten expandiert werden.\nDabei machen wir uns eine spezielle Eigenschaft des verwendeten Embedding-Modells (jina-embeddings-v3) zunutze: das Matryoshka Representation Learning. Dies erlaubt es uns, die Dimensionen der Vektoren zu kürzen, ohne signifikant an semantischer Information zu verlieren. Anstatt der vollen Länge verwenden wir nur die ersten 64 Dimensionen der Embeddings. Dies reduziert die Komplexität des Modells erheblich und verhindert Overfitting, während die wichtigsten semantischen Informationen erhalten bleiben.\n\ndf_ml = pd.read_parquet(Path(\"../data/spotify_genius_embeddings_v3.parquet\"))\ndf_ml = df_ml.drop(\n    columns=[\n        \"id\",\n        \"name\",\n        \"album\",\n        \"album_id\",\n        \"artists\",\n        \"artist_ids\",\n        \"track_number\",\n        \"disc_number\",\n        \"year\",\n        \"lyrics\",\n    ]\n)\n\n# Expand embedding column into separate columns\nemb_df = pd.DataFrame(df_ml[\"embedding\"].tolist(), index=df_ml.index)\n# Only add first 64 dimensions\nemb_df = emb_df.iloc[:, :64]\nemb_df.columns = [f\"emb_{i}\" for i in range(emb_df.shape[1])]\n\ndf_ml = pd.concat([df_ml.drop(columns=[\"embedding\"]), emb_df], axis=1)\n\nX = df_ml.drop(columns=[\"popularity\"])\ny = df_ml[\"popularity\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=67)\n\nprint(f\"Training Data Shape: {X_train.shape}\")\nprint(f\"Testing Data Shape: {X_test.shape}\")\n\nTraining Data Shape: (377182, 79)\nTesting Data Shape: (94296, 79)\n\n\n\nxgb_model = XGBRegressor(n_estimators=3000, learning_rate=0.1, random_state=67, n_jobs=-1)\nxgb_model.fit(X_train, y_train)\n\nXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             feature_weights=None, gamma=None, grow_policy=None,\n             importance_type=None, interaction_constraints=None,\n             learning_rate=0.1, max_bin=None, max_cat_threshold=None,\n             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n             max_leaves=None, min_child_weight=None, missing=nan,\n             monotone_constraints=None, multi_strategy=None, n_estimators=3000,\n             n_jobs=-1, num_parallel_tree=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBRegressor?Documentation for XGBRegressoriFitted\n        \n            \n                Parameters\n                \n\n\n\n\nobjective \n'reg:squarederror'\n\n\n\nbase_score \nNone\n\n\n\nbooster \nNone\n\n\n\ncallbacks \nNone\n\n\n\ncolsample_bylevel \nNone\n\n\n\ncolsample_bynode \nNone\n\n\n\ncolsample_bytree \nNone\n\n\n\ndevice \nNone\n\n\n\nearly_stopping_rounds \nNone\n\n\n\nenable_categorical \nFalse\n\n\n\neval_metric \nNone\n\n\n\nfeature_types \nNone\n\n\n\nfeature_weights \nNone\n\n\n\ngamma \nNone\n\n\n\ngrow_policy \nNone\n\n\n\nimportance_type \nNone\n\n\n\ninteraction_constraints \nNone\n\n\n\nlearning_rate \n0.1\n\n\n\nmax_bin \nNone\n\n\n\nmax_cat_threshold \nNone\n\n\n\nmax_cat_to_onehot \nNone\n\n\n\nmax_delta_step \nNone\n\n\n\nmax_depth \nNone\n\n\n\nmax_leaves \nNone\n\n\n\nmin_child_weight \nNone\n\n\n\nmissing \nnan\n\n\n\nmonotone_constraints \nNone\n\n\n\nmulti_strategy \nNone\n\n\n\nn_estimators \n3000\n\n\n\nn_jobs \n-1\n\n\n\nnum_parallel_tree \nNone\n\n\n\nrandom_state \n67\n\n\n\nreg_alpha \nNone\n\n\n\nreg_lambda \nNone\n\n\n\nsampling_method \nNone\n\n\n\nscale_pos_weight \nNone\n\n\n\nsubsample \nNone\n\n\n\ntree_method \nNone\n\n\n\nvalidate_parameters \nNone\n\n\n\nverbosity \nNone\n\n\n\n\n            \n        \n    \n\n\nAufgrund der deutlich grösseren Anzahl Features durch die Embeddings stieg die Komplexität des Modells. Nach einigen Tests haben wir die Anzahl der Schätzer daher auf 3000 (n_estimators=3000) erhöht.\n\n\nResultate\nDie Ergebnisse des XGBoost-Modells mit den erweiterten Songtext-Embeddings zeigen eine deutliche Verbesserung. Der \\(R^2\\)-Wert stieg auf 0.226, und der RMSE verbesserte sich auf 11.69. Dies bestätigt unsere Annahme, dass die semantischen Inhalte der Lyrics wertvolle Zusatzinformationen für die Popularitätsvorhersage liefern.\n\ny_pred_xgb = xgb_model.predict(X_test)\nrmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\nr2_xgb = r2_score(y_test, y_pred_xgb)\n\nprint(f\"XGBoost RMSE: {rmse_xgb:.2f}\")\nprint(f\"XGBoost R^2: {r2_xgb:.4f}\")\n\nXGBoost RMSE: 11.69\nXGBoost R^2: 0.2256\n\n\n\n\nModellinterpretation\nDie Feature Importance zeigt nun eine moderate Gewichtung der semantischen Embedding-Features. Dies könnte darauf hindeuten, dass bestimmte semantische Konzepte oder Themen in den Lyrics einen signifikanten Einfluss auf die Popularität haben.\nZusätzlich bleibt die Bedeutung des Veröffentlichungsdatums (track_age_days) und der Lautstärke (loudness) bestehen, was die vorherigen Erkenntnisse bestätigt.\n\n\nCode\nplt.figure(figsize=(10, 8))\nplot_importance(xgb_model, max_num_features=20, height=0.5)\nplt.show()\n\n\n&lt;Figure size 1000x800 with 0 Axes&gt;"
  },
  {
    "objectID": "3_modellierungsbericht.html#fazit",
    "href": "3_modellierungsbericht.html#fazit",
    "title": "Modellierungsbericht",
    "section": "Fazit",
    "text": "Fazit\nDie Integration von Songtext-Embeddings in das XGBoost-Modell lieferte mit einem \\(R^2\\) von 0.242 das beste Ergebnis und übertraf die rein akustischen Modelle deutlich. Dies bestätigt, dass semantische Inhalte neben Audio-Merkmalen eine messbare Rolle für die Popularität spielen. Dennoch bleibt ein grosser Teil der Varianz unerklärt, was darauf hindeutet, dass intrinsische Song-Eigenschaften allein den kommerziellen Erfolg nur bedingt vorhersagen können.\n\n\nCode\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nmodels = [\"Linear Regression\", \"Random Forest\", \"XGBoost (Audio only)\", \"XGBoost (+ Lyrics)\"]\nr2_scores = [0.054, 0.173, 0.174, 0.226]\nrmse_scores = [12.92, 12.08, 12.07, 11.69]\n\n\nfig = make_subplots(specs=[[{\"secondary_y\": True}]])\n\nfig.add_trace(\n    go.Bar(\n        x=models,\n        y=r2_scores,\n        name=\"R^2 Score (höher ist besser)\",\n        marker_color=\"#1f77b4\",\n        text=r2_scores,\n        textposition=\"auto\",\n        texttemplate=\"%{text:.3f}\",\n    ),\n    secondary_y=False,\n)\n\nfig.add_trace(\n    go.Scatter(\n        x=models,\n        y=rmse_scores,\n        name=\"RMSE (tiefer ist besser)\",\n        marker_color=\"#d62728\",\n        mode=\"lines+markers+text\",\n        text=rmse_scores,\n        textposition=\"top center\",\n        texttemplate=\"%{text:.2f}\",\n    ),\n    secondary_y=True,\n)\n\nfig.update_layout(\n    title=\"Modellvergleich: R^2 vs. RMSE\",\n    legend={\"orientation\": \"h\", \"yanchor\": \"bottom\", \"y\": 1.02, \"xanchor\": \"right\", \"x\": 1},\n    height=500,\n)\n\nfig.update_yaxes(title_text=\"R^2 Score\", secondary_y=False, range=[0, 0.23])\nfig.update_yaxes(title_text=\"RMSE\", secondary_y=True, range=[0, 14])\n\nfig.show()"
  },
  {
    "objectID": "3_modellierungsbericht.html#limitierungen",
    "href": "3_modellierungsbericht.html#limitierungen",
    "title": "Modellierungsbericht",
    "section": "Limitierungen",
    "text": "Limitierungen\nTrotz der Reduktion der Embedding-Dimensionen besteht bei der hohen Anzahl an Features ein Risiko für Overfitting, insbesondere bei selteneren Genres oder Sprachen. Die grösste Limitierung liegt jedoch in der Natur der Sache: Die Popularität eines Songs wird massgeblich von externen Faktoren bestimmt, die unser Datensatz nicht abbildet. Dazu gehören Marketingbudgets, die bestehende Bekanntheit der Künstler:innen, Social-Media-Trends (z.B. TikTok) oder strategische Playlist-Platzierungen durch Labels. Ein Modell, das “blind” für diesen Kontext ist, stösst an natürliche Grenzen."
  },
  {
    "objectID": "3_modellierungsbericht.html#nächste-schritte",
    "href": "3_modellierungsbericht.html#nächste-schritte",
    "title": "Modellierungsbericht",
    "section": "Nächste Schritte",
    "text": "Nächste Schritte\nUm die Vorhersagekraft signifikant zu steigern, müssten externe Datenquellen integriert werden:\n\nKünstler-Metriken: Historische Popularität, Follower-Zahlen oder Social-Media-Engagement\nKontext-Daten: Informationen über Playlist-Inklusionen oder Radio-Airplay\nDeep Audio: Analyse von Roh-Audiodateien (Spektrogramme) mittels CNNs statt nur aggregierter Features"
  },
  {
    "objectID": "4_evaluationsbericht.html",
    "href": "4_evaluationsbericht.html",
    "title": "Evaluationsbericht",
    "section": "",
    "text": "Die in der Projektcharta definierten quantitativen Erfolgsmetriken wurden mit dem finalen Modell (XGBoost mit Songtext-Embeddings) gemischt erreicht.\n\nMetrik \\(R^2\\) (Erklärungsgehalt):\n\nZielwert: ≥ 0.4\nErreicht: 0.242\n\nBeurteilung: Ziel verfehlt. Das Modell kann rund 24% der Varianz in der Popularität erklären. Dies ist eine deutliche Steigerung gegenüber der Baseline (5.4%), bleibt jedoch unter dem angestrebten Zielwert von 40%. Dies deutet darauf hin, dass ein erheblicher Teil der Popularität nicht durch intrinsische Song-Eigenschaften (Audio & Text) erklärt werden kann.\n\nMetrik RMSE (Prognosegenauigkeit):\n\nZielwert: ≤ 15 (Projektziel) bzw. ≤ 10 (Data Mining Ziel)\nErreicht: 11.69\nBeurteilung: Teilweise erreicht. Das übergeordnete Projektziel einer durchschnittlichen Abweichung von maximal 15 Punkten auf der Popularitätsskala wurde erfolgreich unterboten. Das ambitioniertere technische Ziel von ≤ 10 wurde jedoch knapp verfehlt.\n\n\n\n\n\nDas primäre Projektziel war es, dem Talent Manager und den Künstlern nützliche Prognosen zu liefern, um das Erfolgspotenzial von Songs einzuschätzen.\nErfüllung der Anforderungen: Das entwickelte Modell liefert mit einem RMSE von 11.69 eine solide Indikation für die zu erwartende Popularität. Es eignet sich gut, um “tote” Tracks (ohne Erfolgsaussichten) frühzeitig zu identifizieren und vom manuellen Review-Prozess auszuschliessen.\nIdentifizierte Lücken: Die Analyse hat gezeigt, dass rein musikalische und textliche Inhalte den kommerziellen Erfolg nur bedingt vorhersagen können. Wichtige externe Einflussfaktoren, die in der Realität massgeblich für einen Hit verantwortlich sind - wie Marketingbudget, Social-Media-Trends (z.B. TikTok) oder die Bekanntheit des Künstlers - konnten im aktuellen Modell aufgrund fehlender Daten nicht berücksichtigt werden.\nFazit für den Auftraggeber: Das Tool bietet eine wertvolle, datenbasierte Zweitmeinung und Filterfunktion, sollte jedoch nicht als alleiniges Entscheidungskriterium für Investitionen genutzt werden, sondern als Ergänzung zur menschlichen Expertise des Talent Managers."
  },
  {
    "objectID": "4_evaluationsbericht.html#data-mining--modellierungs-ziele",
    "href": "4_evaluationsbericht.html#data-mining--modellierungs-ziele",
    "title": "Evaluationsbericht",
    "section": "",
    "text": "Die in der Projektcharta definierten quantitativen Erfolgsmetriken wurden mit dem finalen Modell (XGBoost mit Songtext-Embeddings) gemischt erreicht.\n\nMetrik \\(R^2\\) (Erklärungsgehalt):\n\nZielwert: ≥ 0.4\nErreicht: 0.242\n\nBeurteilung: Ziel verfehlt. Das Modell kann rund 24% der Varianz in der Popularität erklären. Dies ist eine deutliche Steigerung gegenüber der Baseline (5.4%), bleibt jedoch unter dem angestrebten Zielwert von 40%. Dies deutet darauf hin, dass ein erheblicher Teil der Popularität nicht durch intrinsische Song-Eigenschaften (Audio & Text) erklärt werden kann.\n\nMetrik RMSE (Prognosegenauigkeit):\n\nZielwert: ≤ 15 (Projektziel) bzw. ≤ 10 (Data Mining Ziel)\nErreicht: 11.69\nBeurteilung: Teilweise erreicht. Das übergeordnete Projektziel einer durchschnittlichen Abweichung von maximal 15 Punkten auf der Popularitätsskala wurde erfolgreich unterboten. Das ambitioniertere technische Ziel von ≤ 10 wurde jedoch knapp verfehlt."
  },
  {
    "objectID": "4_evaluationsbericht.html#projektziele",
    "href": "4_evaluationsbericht.html#projektziele",
    "title": "Evaluationsbericht",
    "section": "",
    "text": "Das primäre Projektziel war es, dem Talent Manager und den Künstlern nützliche Prognosen zu liefern, um das Erfolgspotenzial von Songs einzuschätzen.\nErfüllung der Anforderungen: Das entwickelte Modell liefert mit einem RMSE von 11.69 eine solide Indikation für die zu erwartende Popularität. Es eignet sich gut, um “tote” Tracks (ohne Erfolgsaussichten) frühzeitig zu identifizieren und vom manuellen Review-Prozess auszuschliessen.\nIdentifizierte Lücken: Die Analyse hat gezeigt, dass rein musikalische und textliche Inhalte den kommerziellen Erfolg nur bedingt vorhersagen können. Wichtige externe Einflussfaktoren, die in der Realität massgeblich für einen Hit verantwortlich sind - wie Marketingbudget, Social-Media-Trends (z.B. TikTok) oder die Bekanntheit des Künstlers - konnten im aktuellen Modell aufgrund fehlender Daten nicht berücksichtigt werden.\nFazit für den Auftraggeber: Das Tool bietet eine wertvolle, datenbasierte Zweitmeinung und Filterfunktion, sollte jedoch nicht als alleiniges Entscheidungskriterium für Investitionen genutzt werden, sondern als Ergänzung zur menschlichen Expertise des Talent Managers."
  },
  {
    "objectID": "4_evaluationsbericht.html#erweiterung-des-domänenwissens",
    "href": "4_evaluationsbericht.html#erweiterung-des-domänenwissens",
    "title": "Evaluationsbericht",
    "section": "Erweiterung des Domänenwissens",
    "text": "Erweiterung des Domänenwissens\nDa intrinsische Song-Eigenschaften allein den Erfolg nur begrenzt erklären, sollte der Fokus verstärkt auf externe Marktdynamiken wie Marketingbudgets und Social-Media-Trends (z.B. TikTok) gelegt werden."
  },
  {
    "objectID": "4_evaluationsbericht.html#adaption-der-projektziele",
    "href": "4_evaluationsbericht.html#adaption-der-projektziele",
    "title": "Evaluationsbericht",
    "section": "Adaption der Projektziele",
    "text": "Adaption der Projektziele\nDas Ziel einer exakten Vorhersage sollte zugunsten eines Filterinstruments angepasst werden. Das Modell eignet sich primär zur “Negative Selection”, um Songs ohne Potenzial frühzeitig auszusortieren, statt exakte Hit-Scores zu garantieren."
  },
  {
    "objectID": "4_evaluationsbericht.html#datenerweiterung",
    "href": "4_evaluationsbericht.html#datenerweiterung",
    "title": "Evaluationsbericht",
    "section": "Datenerweiterung",
    "text": "Datenerweiterung\nIntegration externer Datenquellen zur Steigerung der Vorhersagekraft:\n\nKünstler-Metriken: Historische Daten wie Follower-Zahlen und vergangene Chart-Erfolge.\nKontext-Daten: Informationen über Playlist-Inklusionen oder Radio-Airplay.\nErweiterte Audio-Features: Nutzung von AcousticBrainz, um detaillierte Low-Level-Spektraldaten sowie Stimmungs- und Genre-Informationen (via Essentia Toolkit) zu erhalten, die über die Standard-Spotify-Features hinausgehen."
  },
  {
    "objectID": "4_evaluationsbericht.html#rechtliche-und-ethische-aspekte",
    "href": "4_evaluationsbericht.html#rechtliche-und-ethische-aspekte",
    "title": "Evaluationsbericht",
    "section": "Rechtliche und ethische Aspekte",
    "text": "Rechtliche und ethische Aspekte\n\nPlattform-Risiko: Die Abhängigkeit von den Spotify-Nutzungsbedingungen muss überwacht werden.\nBias: Der Bias der Zielvariable “Popularity” hin zu Mainstream-Musik und Songs mit verfügbaren Lyrics (Genius) ist bei der Anwendung zu berücksichtigen."
  },
  {
    "objectID": "4_evaluationsbericht.html#bereitstellung",
    "href": "4_evaluationsbericht.html#bereitstellung",
    "title": "Evaluationsbericht",
    "section": "Bereitstellung",
    "text": "Bereitstellung\n\nNutzung: Die Resultate dienen dem Talent Manager als operative Entscheidungsunterstützung (Zweitmeinung) zur Priorisierung von Songs.\nForm: Übergabe der Software als dokumentierte Jupyter Notebooks an das IT-Team zur technischen Integration."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Song Popularity Predictor",
    "section": "",
    "text": "Dieses Projekt wurde im Rahmen vom DS.DSG Modul durchgeführt und hat zum Ziel, die Popularität von Songs mithilfe datengetriebener Methoden nachvollziehbar vorherzusagen. Dazu werden verschiedene Informationsquellen kombiniert - Metadaten, audio-basierte Features und textuelle Repräsentationen (z.B. Lyrics-Embeddings).\nDie Arbeit legt Wert auf Transparenz, Reproduzierbarkeit und nachvollziehbare Evaluationsmetriken, sodass Modellentscheidungen und Ergebnisse geprüft werden können.\n\n\n\nDaten: Metadaten, Audio-Features, Lyrics-Embeddings und ergänzende Variablen\nZiel: Entwicklung robuster Vorhersagemodelle und verständliche Evaluationen\nStruktur: Die Hauptinhalte liegen als Quarto-Site im Ordner site/ und sind in mehrere Jupyter-Notebooks gegliedert:\n\nProjektcharta - Projektziele und Vorgehen\nDatenbericht - Datenexploration und Aufbereitung\nModellierungsbericht - Modellaufbau und Training\nEvaluationsbericht - Evaluation und Ergebnisse\n\n\n\n\n\nChatGPT, Google Gemini und Copilot (via VS-Code) wurden für folgende Zwecke selektiv eingesetzt:\n\nSprachliche Überarbeitung der Dokumentation\nDebugging / Troubleshooting\n\nDie fachliche Konzeption, Datenanalyse, Modellierung, Interpretation der Ergebnisse sowie die inhaltliche Verantwortung wurden vollständig eigenständig übernommen.\nSämtliche durch KI erzeugten Inhalte wurden kritisch geprüft und gegebenenfalls angepasst."
  },
  {
    "objectID": "index.html#projektüberblick",
    "href": "index.html#projektüberblick",
    "title": "Song Popularity Predictor",
    "section": "",
    "text": "Daten: Metadaten, Audio-Features, Lyrics-Embeddings und ergänzende Variablen\nZiel: Entwicklung robuster Vorhersagemodelle und verständliche Evaluationen\nStruktur: Die Hauptinhalte liegen als Quarto-Site im Ordner site/ und sind in mehrere Jupyter-Notebooks gegliedert:\n\nProjektcharta - Projektziele und Vorgehen\nDatenbericht - Datenexploration und Aufbereitung\nModellierungsbericht - Modellaufbau und Training\nEvaluationsbericht - Evaluation und Ergebnisse"
  },
  {
    "objectID": "index.html#ki-deklaraion",
    "href": "index.html#ki-deklaraion",
    "title": "Song Popularity Predictor",
    "section": "",
    "text": "ChatGPT, Google Gemini und Copilot (via VS-Code) wurden für folgende Zwecke selektiv eingesetzt:\n\nSprachliche Überarbeitung der Dokumentation\nDebugging / Troubleshooting\n\nDie fachliche Konzeption, Datenanalyse, Modellierung, Interpretation der Ergebnisse sowie die inhaltliche Verantwortung wurden vollständig eigenständig übernommen.\nSämtliche durch KI erzeugten Inhalte wurden kritisch geprüft und gegebenenfalls angepasst."
  },
  {
    "objectID": "1_projektcharta.html",
    "href": "1_projektcharta.html",
    "title": "Projektcharta",
    "section": "",
    "text": "Die ZHAW School of Engineering hat kürzlich ihr Musiklabel “Bandwidth Beats” gegründet. Um erfolgreich durchstarten zu können, benötigt das Label aktuelle Einblicke in die Musikszene. Ziel ist es, zu verstehen, was einen populären Song ausmacht, und diese Erkenntnisse datenbasiert in Entscheidungsprozesse einfliessen zu lassen.\nDas Projekt soll von einem Team aus Data-Science-Studierenden umgesetzt werden. Das entstehende Datenprodukt speist die Songdaten der Künstlerinnen und Künstler des Labels ein und erstellt Prognosen zum potenziellen Erfolg von Songs. Im Vergleich zu bestehenden Konkurrenzprodukten ist diese Lösung insbesondere für kleinere Labels zugänglich und zeichnet sich durch eine höhere Flexibilität aus.\n\n\n\nDie Nutzer:innen-Analyse zeigt, dass insbesondere kleinere Musiklabels Wert auf eine möglichst einfache Anwendbarkeit der Lösung legen, ohne umfangreiche technische Vorkenntnisse oder komplexe Prozesse. Ein zentraler Bedarf besteht darin, mit minimalen Eingangsdaten aussagekräftige Analysen und Prognosen zu erhalten. Viele bestehende Ansätze setzen umfangreiche und teilweise schwer verfügbare Datensätze voraus, was für kleinere Labels eine hohe Einstiegshürde darstellt. Entsprechend wird eine datenarme, dennoch robuste Lösung als wesentlicher Mehrwert wahrgenommen.\n\n\n\n\n\n\nAbbildung 1: Stakeholdermap\n\n\n\n\nDer Projektsponsor ist das Präsidium des Musiklabels Bandwidth Beats. Ziel des Auftraggebers ist es, durch den Einsatz von Data Science innovative Entscheidungsgrundlagen für das Label zu schaffen und gleichzeitig den Praxisbezug in der Lehre zu stärken. Der Auftraggeber definiert den Projektauftrag, stellt die Rahmenbedingungen bereit und steht in direkter Beziehung zum Projektteam sowie zum Talent Manager.\n\n\n\nDer Talent Manager verfolgt das Ziel, Künstler:innen datenbasiert zu bewerten und deren Erfolgspotenzial frühzeitig einzuschätzen. Die Projektergebnisse dienen ihm als operative Entscheidungsunterstützung bei der Auswahl, Förderung und Priorisierung von Künstler:innen und Songs. Er steht in engem Austausch mit dem Auftraggeber sowie mit den Künstler:innen und Agent:innen und ist ein zentraler Nutzer des entwickelten Datenprodukts.\n\n\n\nDas IT-Team ist für die technische Umsetzbarkeit, Integration und gegebenenfalls den Betrieb der Lösung zuständig. Ziel ist es, eine stabile, wartbare und datenschutzkonforme technische Grundlage sicherzustellen. Das IT-Team arbeitet eng mit dem Projektteam zusammen und koordiniert sich mit dem Auftraggeber, um technische Anforderungen und organisatorische Vorgaben abzugleichen.\n\n\n\nDie Künstler:innen sind betroffene Stakeholder, deren Songdaten in das Datenprodukt einfliessen. Ihr Ziel ist eine faire und transparente Bewertung ihres Erfolgspotenzials sowie eine gezielte Förderung durch das Label. Sie stehen in Beziehung zum Talent Manager und zu den Agent:innen; direkte Interaktionen mit dem Projektteam finden in der Regel nicht statt.\n\n\n\nAgent:innen vertreten die Interessen der Künstler:innen und verfolgen das Ziel, deren Marktposition und Erfolgschancen zu maximieren. Sie sind indirekt vom Projekt betroffen, da die datenbasierten Entscheidungen des Labels Auswirkungen auf Vertragsgestaltung, Promotion und Investitionen haben können. Agent:innen stehen in enger Beziehung zu den Künstler:innen und im Austausch mit dem Talent Manager."
  },
  {
    "objectID": "1_projektcharta.html#ausgangslage",
    "href": "1_projektcharta.html#ausgangslage",
    "title": "Projektcharta",
    "section": "",
    "text": "Die ZHAW School of Engineering hat kürzlich ihr Musiklabel “Bandwidth Beats” gegründet. Um erfolgreich durchstarten zu können, benötigt das Label aktuelle Einblicke in die Musikszene. Ziel ist es, zu verstehen, was einen populären Song ausmacht, und diese Erkenntnisse datenbasiert in Entscheidungsprozesse einfliessen zu lassen.\nDas Projekt soll von einem Team aus Data-Science-Studierenden umgesetzt werden. Das entstehende Datenprodukt speist die Songdaten der Künstlerinnen und Künstler des Labels ein und erstellt Prognosen zum potenziellen Erfolg von Songs. Im Vergleich zu bestehenden Konkurrenzprodukten ist diese Lösung insbesondere für kleinere Labels zugänglich und zeichnet sich durch eine höhere Flexibilität aus."
  },
  {
    "objectID": "1_projektcharta.html#erkenntnisse-aus-der-nutzer-analyse",
    "href": "1_projektcharta.html#erkenntnisse-aus-der-nutzer-analyse",
    "title": "Projektcharta",
    "section": "",
    "text": "Die Nutzer:innen-Analyse zeigt, dass insbesondere kleinere Musiklabels Wert auf eine möglichst einfache Anwendbarkeit der Lösung legen, ohne umfangreiche technische Vorkenntnisse oder komplexe Prozesse. Ein zentraler Bedarf besteht darin, mit minimalen Eingangsdaten aussagekräftige Analysen und Prognosen zu erhalten. Viele bestehende Ansätze setzen umfangreiche und teilweise schwer verfügbare Datensätze voraus, was für kleinere Labels eine hohe Einstiegshürde darstellt. Entsprechend wird eine datenarme, dennoch robuste Lösung als wesentlicher Mehrwert wahrgenommen."
  },
  {
    "objectID": "1_projektcharta.html#stakeholder",
    "href": "1_projektcharta.html#stakeholder",
    "title": "Projektcharta",
    "section": "",
    "text": "Abbildung 1: Stakeholdermap\n\n\n\n\nDer Projektsponsor ist das Präsidium des Musiklabels Bandwidth Beats. Ziel des Auftraggebers ist es, durch den Einsatz von Data Science innovative Entscheidungsgrundlagen für das Label zu schaffen und gleichzeitig den Praxisbezug in der Lehre zu stärken. Der Auftraggeber definiert den Projektauftrag, stellt die Rahmenbedingungen bereit und steht in direkter Beziehung zum Projektteam sowie zum Talent Manager.\n\n\n\nDer Talent Manager verfolgt das Ziel, Künstler:innen datenbasiert zu bewerten und deren Erfolgspotenzial frühzeitig einzuschätzen. Die Projektergebnisse dienen ihm als operative Entscheidungsunterstützung bei der Auswahl, Förderung und Priorisierung von Künstler:innen und Songs. Er steht in engem Austausch mit dem Auftraggeber sowie mit den Künstler:innen und Agent:innen und ist ein zentraler Nutzer des entwickelten Datenprodukts.\n\n\n\nDas IT-Team ist für die technische Umsetzbarkeit, Integration und gegebenenfalls den Betrieb der Lösung zuständig. Ziel ist es, eine stabile, wartbare und datenschutzkonforme technische Grundlage sicherzustellen. Das IT-Team arbeitet eng mit dem Projektteam zusammen und koordiniert sich mit dem Auftraggeber, um technische Anforderungen und organisatorische Vorgaben abzugleichen.\n\n\n\nDie Künstler:innen sind betroffene Stakeholder, deren Songdaten in das Datenprodukt einfliessen. Ihr Ziel ist eine faire und transparente Bewertung ihres Erfolgspotenzials sowie eine gezielte Förderung durch das Label. Sie stehen in Beziehung zum Talent Manager und zu den Agent:innen; direkte Interaktionen mit dem Projektteam finden in der Regel nicht statt.\n\n\n\nAgent:innen vertreten die Interessen der Künstler:innen und verfolgen das Ziel, deren Marktposition und Erfolgschancen zu maximieren. Sie sind indirekt vom Projekt betroffen, da die datenbasierten Entscheidungen des Labels Auswirkungen auf Vertragsgestaltung, Promotion und Investitionen haben können. Agent:innen stehen in enger Beziehung zu den Künstler:innen und im Austausch mit dem Talent Manager."
  },
  {
    "objectID": "1_projektcharta.html#ressourcen",
    "href": "1_projektcharta.html#ressourcen",
    "title": "Projektcharta",
    "section": "Ressourcen",
    "text": "Ressourcen\n\nPersonal:\n\n3 Data Science-Studierende\nManuel Dömer (Coach)\n\nSoftware:\n\nZHAW-lizenzierte Software zur Datenspeicherung, Kommunikation und Projekt-Management\nInnerhalb des Kurses “Data Science Grundlagen” bereitgestellte Tools zur Datenanalyse und -modellierung\n3x $100 Microsoft Azure Edu-Credits\n\nZeit:\n\n3 Wochen"
  },
  {
    "objectID": "1_projektcharta.html#einschränkungen-randbedingungen",
    "href": "1_projektcharta.html#einschränkungen-randbedingungen",
    "title": "Projektcharta",
    "section": "Einschränkungen & Randbedingungen",
    "text": "Einschränkungen & Randbedingungen\n\nMinimale (Roh-) Inputdaten\nSpotify als Datengrundlage"
  },
  {
    "objectID": "1_projektcharta.html#risiken",
    "href": "1_projektcharta.html#risiken",
    "title": "Projektcharta",
    "section": "Risiken",
    "text": "Risiken\n\nUngenügende Datentiefe: Es können nicht genug Faktoren für die Popularität eines Songs identifiziert werden\nBias: Popularity-Score wurde von Spotify berechnet und ist evtl. nicht universell anwendbar\nAbhängigkeit von Spotify: Nutzungsbedingungen können sich Ändern"
  },
  {
    "objectID": "1_projektcharta.html#phase-1-aufgabendefinition",
    "href": "1_projektcharta.html#phase-1-aufgabendefinition",
    "title": "Projektcharta",
    "section": "Phase 1: Aufgabendefinition",
    "text": "Phase 1: Aufgabendefinition\n\nVergleichbare öffentliche Projekte analysieren\nRessourcen abklären\nStakeholder analysieren\nProjektziele und Erfolgskriterien festlegen\nModellierungsziele ableiten\nProjektplan erstellen"
  },
  {
    "objectID": "1_projektcharta.html#phase-2-datenbeschaffung",
    "href": "1_projektcharta.html#phase-2-datenbeschaffung",
    "title": "Projektcharta",
    "section": "Phase 2: Datenbeschaffung",
    "text": "Phase 2: Datenbeschaffung\n\nDatenquellen identifizieren\nDaten beschaffen\nDaten ablegen und organisieren\nDatenkatalog erstellen\nExplorative Datenanalyse"
  },
  {
    "objectID": "1_projektcharta.html#phase-3-modellierung",
    "href": "1_projektcharta.html#phase-3-modellierung",
    "title": "Projektcharta",
    "section": "Phase 3: Modellierung",
    "text": "Phase 3: Modellierung\n\nNotwendige Datentransformation durchführen\nData Mining Algorithmen anwenden und Resultate vergleichen"
  },
  {
    "objectID": "1_projektcharta.html#phase-4-evaluation",
    "href": "1_projektcharta.html#phase-4-evaluation",
    "title": "Projektcharta",
    "section": "Phase 4: Evaluation",
    "text": "Phase 4: Evaluation\n\nResultate interpretieren und bewerten\nPräsentation der Resultate an die Stakeholder & Entscheid"
  },
  {
    "objectID": "1_projektcharta.html#phase-5-bereitstellung",
    "href": "1_projektcharta.html#phase-5-bereitstellung",
    "title": "Projektcharta",
    "section": "Phase 5: Bereitstellung",
    "text": "Phase 5: Bereitstellung\n\nÜbergabe der Software (inkl. Dokumentation) an das IT-Team des Kunden\nVerhandlung der Support-Konditionen"
  },
  {
    "objectID": "1_projektcharta.html#entwickler",
    "href": "1_projektcharta.html#entwickler",
    "title": "Projektcharta",
    "section": "Entwickler",
    "text": "Entwickler\nDie Entwickler / Data Scientists halten alle zwei Tage ein Teams-Meetings um den Fortschritt und allfällige Probleme zu besprechen."
  },
  {
    "objectID": "1_projektcharta.html#stakeholder-1",
    "href": "1_projektcharta.html#stakeholder-1",
    "title": "Projektcharta",
    "section": "Stakeholder",
    "text": "Stakeholder\nDie Stakeholder, insbesondere der Sponsor wird wöchentlich vom Teamsprecher über den Projektverlauf informiert."
  },
  {
    "objectID": "2_datenbericht.html",
    "href": "2_datenbericht.html",
    "title": "Datenbericht",
    "section": "",
    "text": "Übersichtstabelle der Rohdatensätze\n\n\nDatensatz Name\nQuelle\nSpeicherort\n\n\n\n\nSpotify 1.2M+ Songs\nKaggle\ndata/tracks_features.csv\n\n\nSpotify API (GET /tracks)\nSpotify API\ndata/raw_responses/batch_{i}.json\n\n\nGenius Song Lyrics\nKaggle\ndata/song_lyrics.csv\n\n\n\n\n\nDie Grundbasis dieser Arbeit ist das “Spotify 1.2M+ Songs” Dataset von Rodolfo Figueroa auf kaggle.com. Enthalten ist eine CSV-Datei, tracks_features.csv, welche Daten zu über 1.2 Millionen Songs auf Spotify von der Spotify Developer API enthalten. Spezifisch von den /tracks und /audio-features endpoints.\nAudio Features sind Informationen über ein Lied, welche zum einen direkt dem Lied entnommen wurden, wie z.B. duration_ms, tempo und key, sowohl auch Werte welche durch eine Analyse von Spotify kalkuliert wurden (z.B. speechiness, valence und energy).\nLeider fehlt in diesem Dataset eine wichtige Kennzahl, popularity. Deswegen haben wir diese Zahlen selber anhand der Songs im Dataset von der API entnommen.\nMithilfe eines API-Accounts und dem folgenden Python Skript haben wir dies erreicht. Es..\n\nliest alle Spotify Track IDs des tracks_features.csv ein\nfragt den API endpoint tracks in Batches von 50 Tracks ab\nspeichert die Responses pro Batch im directory data/raw_responses als batch_{i}.json ab.\n\n\nimport json\nimport math\nimport os\nimport time\nfrom pathlib import Path\n\nimport pandas as pd\nimport requests as rq\nfrom dotenv import load_dotenv\nfrom tqdm import tqdm\n\nload_dotenv()\n\nCLIENT_ID = os.getenv(\"SPOTIFY_CLIENT_ID\")\nCLIENT_SECRET = os.getenv(\"SPOTIFY_CLIENT_SECRET\")\nOUTPUT_DIR = Path(\"data/raw_responses\")\nBATCH_SIZE = 50\n\n\ndef get_access_token() -&gt; str:\n    auth_url = \"https://accounts.spotify.com/api/token\"\n    try:\n        auth_response = rq.post(\n            auth_url, {\"grant_type\": \"client_credentials\", \"client_id\": CLIENT_ID, \"client_secret\": CLIENT_SECRET}\n        )\n        auth_response.raise_for_status()\n        return auth_response.json()[\"access_token\"]\n    except Exception as e:\n        print(f\"Failed to get access token: {e}\")\n        raise\n\n\ndef main() -&gt; None:\n    df = pd.read_csv(\"data/tracks_features.csv\")\n    track_ids = df[\"id\"].dropna().unique().tolist()\n\n    total_tracks = len(track_ids)\n    print(f\"Found {total_tracks} tracks.\")\n\n    token = get_access_token()\n    headers = {\"Authorization\": f\"Bearer {token}\"}\n\n    num_batches = math.ceil(total_tracks / BATCH_SIZE)\n\n    for i in tqdm(range(num_batches), desc=\"Fetching batches\"):\n        batch_file = OUTPUT_DIR / f\"batch_{i}.json\"\n\n        # Skip if already processed\n        if batch_file.exists():\n            continue\n\n        start_idx = i * BATCH_SIZE\n        end_idx = start_idx + BATCH_SIZE\n        batch_ids = track_ids[start_idx:end_idx]\n\n        ids_param = \",\".join(batch_ids)\n        url = f\"https://api.spotify.com/v1/tracks?ids={ids_param}\"\n\n        while True:\n            try:\n                time.sleep(2)\n                response = rq.get(url, headers=headers)\n\n                if response.status_code == 200:\n                    data = response.json()\n                    with batch_file.open(\"w\") as f:\n                        json.dump(data, f)\n                    break\n\n                if response.status_code == 429:\n                    retry_after = int(response.headers.get(\"Retry-After\", 5))\n                    tqdm.write(f\"Rate limited. Sleeping for {retry_after} seconds.\")\n                    time.sleep(retry_after + 1)\n\n                elif response.status_code == 401:\n                    tqdm.write(\"Token expired. Refreshing...\")\n                    token = get_access_token()\n                    headers = {\"Authorization\": f\"Bearer {token}\"}\n\n                else:\n                    tqdm.write(f\"Error {response.status_code} for batch {i}: {response.text}\")\n                    # Log error and skip this batch to avoid blocking\n                    error_file = OUTPUT_DIR / f\"error_batch_{i}.txt\"\n                    with error_file.open(\"w\") as f:\n                        f.write(f\"Status: {response.status_code}\\n{response.text}\")\n                    break\n\n            except rq.exceptions.RequestException as e:\n                tqdm.write(f\"Request exception: {e}\")\n                time.sleep(5)\n\n\nif __name__ == \"__main__\":\n    main()\n\nDanach hatten wir 24’081 JSON files, die wir mit dem tracks_features.csv Dataset joinen mussten. Dies haben wir mit dem folgenden Skript vollendet. Aus Performance Gründen haben wir das orjson Python package und eingebautes Multiprocessing verwendet:\n\nfrom concurrent.futures import ThreadPoolExecutor\nfrom pathlib import Path\n\nimport orjson\n\n# load all json files in data/raw_responses and combine into a single dataframe\ndata_path = Path(\"../data/raw_responses\")\njson_files = list(data_path.glob(\"*.json\"))\nprint(f\"Found {len(json_files)} JSON files.\")\n\n\ndef process_file(file_path: Path) -&gt; list:\n    data = orjson.loads(file_path.open(\"rb\").read())\n    tracks = []\n    for t in data.get(\"tracks\", []):\n        t.pop(\"available_markets\", None)\n        tracks.append(t)\n    return tracks\n\n\nprint(\"Processing files in parallel...\")\ntracks_all = []\nwith ThreadPoolExecutor() as executor:\n    results = executor.map(process_file, json_files)\n\n    for tracks in tqdm(results, total=len(json_files)):\n        tracks_all.extend(tracks)\n\ndf = pd.DataFrame(tracks_all)\nprint(f\"Total tracks loaded: {len(df)}\")\n\nFound 24081 JSON files.\nProcessing files in parallel...\n\n\n100%|██████████| 24081/24081 [02:19&lt;00:00, 172.81it/s]\n\n\nTotal tracks loaded: 1204025\n\n\n\ndf_features = pd.read_csv(\"../data/tracks_features.csv\", keep_default_na=False)\nprint(f\"Tracks geladen: {len(df_features)}\")\n\ndf_combined = df_features.merge(df[[\"id\", \"popularity\"]], on=\"id\", how=\"left\")\nprint(f\"Kombinierte shape: {df_combined.shape}\")\n\n# How many rows with missing popularity?\nmissing_popularity = df_combined[\"popularity\"].isna().sum()\nprint(f\"Zeilen mit fehlender popularity: {missing_popularity}\")\n\n# Export to CSV\noutput_path = Path(\"../data/full_tracks_features.csv\")\ndf_combined.to_csv(output_path, index=False)\n\nTracks geladen: 1204025\nKombinierte shape: (1204025, 25)\nZeilen mit fehlender popularity: 0\n\n\nNun hatten wir die popularity Metrik für alle 1’204’025 Tracks hinzugefügt und als eine neue Datei full_tracks_features.csv abgepeichert.\n\n\n\nKaggle-Dataset: Der verwendete Datensatz weist keine explizite eigene Lizenz aus. Laut Beschreibung des Autors wurden die enthaltenen Daten über die Spotify Web API erhoben.\nSpotify API: Die Nutzung der Spotify Web API unterliegt den Spotify Developer Terms (Version 10, gültig seit 15. Mai 2025).\n\nFair Use: Der Zugriff auf die Spotify API erfolgt unter Einhaltung der vorgegebenen Rate Limits. Die API-Zugangsdaten werden sicher verwaltet und nicht öffentlich zugänglich gemacht.\nDatenschutz: Es werden keine personenbezogenen Daten von Spotify-Nutzern verarbeitet.\nUrheberrecht: Es werden keine urheberrechtlich geschützte Inhalte bezogen.\nMachine-Learning: Die eingesetzten Machine-Learning-Verfahren dienen ausschliesslich der statistischen Modellierung und Analyse dieser Metadaten im Rahmen eines Nicht-kommerziellen Schulprojekts.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndex\nName\nDatentyp\nWerte\nBeschreibung\n\n\n\n\n1\nid\nstring\nFormat: base-62\nDie Spotify-ID für den Track\n\n\n2\nname\nstring\n-\nDer Name des Tracks\n\n\n3\nalbum\nstring\n-\nDas Album, auf dem der Track erscheint\n\n\n4\nalbum_id\nstring\nFormat: base-62\nDie Spotify-ID für das Album\n\n\n5\nartists\nListe von strings\n-\nDie Künstler, die den Track performt haben\n\n\n6\nartist_id\nListe von strings\nFormat: base-62\nDie Spotify-ID für die Künstler\n\n\n7\ntrack_number\ninteger\nWertebereich: 1 - 50\nDie Nummer des Tracks auf dem Album\n\n\n8\ndisc_number\ninteger\nWertebereich: 1 - 13\nDie Disc-Nummer, auf dem der Track erscheint\n\n\n9\nexplicit\nboolean\ntrue = Jafalse = Nein oder unbekannt\nOb der Track explizite Texte enthält\n\n\n10\ndanceability\nfloat\nWertebereich: 0 - 1\nTanzbarkeit beschreibt, wie geeignet ein Track zum Tanzen ist, basierend auf einer Kombination musikalischer Elemente.0.0 → am wenigsten tanzbar, 1.0 → am tanzbarsten\n\n\n11\nenergy\nfloat\nWertebereich: 0 - 1\nWahrnehmungsmass für Intensität und Aktivität dar, typischerweise fühlen sich energiegeladene Tracks schnell, laut und geräuschvoll an\n\n\n12\nkey\ninteger\nWertebereich: -1 - 11\nDie Tonart, in der sich der Track befindet, basierend auf Standard-Pitch-Class-Notation, Wert -1 = keine Tonart erkannt\n\n\n13\nloudness\nfloat\nWertebereich: -60 - 0Einheit: Dezibel (dB)\nDie Gesamtlautstärke eines Tracks in Dezibel (dB)\n\n\n14\nmode\ninteger\n1 = Major, 0 = Minor\nGibt die Tonalität (Dur oder Moll) eines Tracks an\n\n\n15\nspeechiness\nfloat\nWertebereich: 0 - 1\nErkennt das Vorhandensein von gesprochenen Worten in einem Track\n\n\n16\nacousticness\nfloat\nWertebereich: 0 - 1\nKonfidenzmass ob der Track akustisch ist\n\n\n17\ninstrumentalness\nfloat\nWertebereich: 0 - 1\nSagt voraus, ob ein Track keinen Gesang enthält\n\n\n18\nliveness\nfloat\nWertebereich: 0 - 1\nErkennt die Anwesenheit eines Publikums in der Aufnahme\n\n\n19\nvalence\nfloat\nWertebereich: 0 - 1\nBeschreibt die musikalische Positivität, die von einem Track vermittelt wird\n\n\n20\ntempo\nfloat\nEinheit: beats per minute (BPM)\nDas geschätzte Gesamttempo eines Tracks in Schlägen pro Minute (BPM)\n\n\n21\nduration_ms\nfloat\nEinheit: Millisekunden (ms)\nDie Dauer des Tracks in Millisekunden.\n\n\n22\ntime_signature\ninteger\nWertebereich: 3 - 7\nEine geschätzte Taktart, gibt wie viele Schläge in jedem Takt enthalten sind\n\n\n23\nyear\ninteger\nFormat: YYYY\nDas Jahr des Veröffentlichungsdatums des Tracks\n\n\n24\nrelease_date\nstring\nFormate:YYYY,YYYY-MM,YYYY-MM-DD\nDas Datum, an dem das Album erstmals veröffentlicht wurde. Präzision und somit Format variiert\n\n\n25\npopularity\ninteger\nWertebereich: 0 - 100\nDie Popularität des Tracks, basiert haupstächlich auf Gesamtzahl der Wiedergaben\n\n\n\n(Für zusätzliche Informationen siehe Spotify API Dokumentation)\n\n\n\nDiese Datenanalyse wurde in Python 3.14 mit den pandas und plotly Paketen durchgeführt. Mehr Details können auf dem Repository im pyproject.toml gefunden werden.\n\nÜbersicht Datenqualität\n\n\nAnzahl Spalten\n25\n\n\nAnzahl Zeilen\n1’204’025\n\n\nAnzahl leerer Zellen\n0\n\n\nAnteil (%) leerer Zellen\n0%\n\n\nAnzahl duplizierter Zeilen\n0\n\n\nAnteil (%) duplizierter Zeilen\n0%\n\n\n\n\n\nDa die Strings im CSV nicht wrapped sind mit Anführungszeichen, hat Pandas die Datei am Anfang der Analyse falsch geladen. Denn es gibt ein Album vom Künstler Gupi mit dem Namen “None”. Pandas hat dies standardmässig als einen fehlenden Wert interpretiert. Um dies zu lösen haben wir für die Analyse und das Weiterverarbeiten von nun an die Datei folgendermassen geladen:\ndf = pd.read_csv(Path(\"../data/full_tracks_features.csv\"), keep_default_na=False, na_values=[\"\"])\n\n\n\nÜberaschenderweise gibt es keine duplizierte Zeilen, obwohl (wie die Dokumentation1 erwähnt) die id Spalte einen Track nicht eindeutig identifiziert.\n\n\n\nDie Verteilung der Popularität ist extrem rechtsschief. Wie die folgende Grafik zeigt, hat ein massiver Anteil der Tracks im Datensatz eine Popularität von 0. Da die Anzahl der Tracks mit Popularität 0 die restlichen Werte bei weitem übersteigt, wird hier eine logarithmische Skala für die y-Achse verwendet.\nDiese grosse Menge an Tracks mit dem Wert 0 deutet auf viele inaktive, sehr alte oder extrem nischenhafte Songs hin, die auf der Plattform kaum oder gar nicht gehört werden. Für die spätere Modellierung ist dies ein entscheidender Faktor, da wir entscheiden müssen, ob wir diese “toten” Datenpunkte behalten oder filtern wollen.\n\n\nCode\nfrom pathlib import Path\n\nimport pandas as pd\nimport plotly.express as px\nimport plotly.io as pio\n\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\n\ndf = pd.read_csv(Path(\"../data/full_tracks_features.csv\"), keep_default_na=False, na_values=[\"\"])\n\nfig = px.histogram(x=df[\"popularity\"], log_y=True, title=\"Verteilung der Popularität (log)\", labels={\"x\": \"Popularity\"})\n\nfig.update_layout(yaxis_title=\"Anzahl Tracks\", xaxis_title=\"Popularity (0-100)\", showlegend=False, yaxis={\"dtick\": 1})\nfig.update_traces(marker_line_color=\"black\", marker_line_width=1)\n\nfig.show()\n\npop_0_count = (df[\"popularity\"] == 0).sum()\npop_gt_0_count = (df[\"popularity\"] &gt; 0).sum()\nprint(f\"Anzahl Tracks mit popularity 0:     {pop_0_count}\")\nprint(f\"Anzahl Tracks mit popularity &gt; 0:   {pop_gt_0_count}\")\n\n\n                            \n                                            \n\n\nAnzahl Tracks mit popularity 0:     729167\nAnzahl Tracks mit popularity &gt; 0:   474858\n\n\n\n\n\nDie Verteilung der Dauer der Tracks zeigt ebenfalls interessante Muster. Auch hier verwenden wir eine logarithmische Skala für die y-Achse.\nEs gibt eine Anzahl an Tracks, die sehr kurz sind (unter 30 Sekunden), konkret 7’709 Stück. Dies sind oft Soundeffekte, Intros oder Interludes. Obwohl dies im Vergleich zur Gesamtmenge ein kleiner Anteil ist, filtern wir diese später heraus, um die Datenqualität zu erhöhen. Zusätzlich zeigt die Tabelle unterhalb der Grafik einige Extremwerte am anderen Ende des Spektrums: Tracks mit einer Dauer von über 90 Minuten.\n\n\nCode\nfig = px.histogram(\n    x=df[\"duration_ms\"] / 1000 / 60, log_y=True, title=\"Verteilung der Dauer (log)\", labels={\"x\": \"Dauer (min)\"}\n)\n\nfig.update_layout(yaxis_title=\"Anzahl Tracks\", xaxis_title=\"Dauer (min)\", showlegend=False, yaxis={\"dtick\": 1})\nfig.update_traces(marker_line_color=\"black\", marker_line_width=1, xbins={\"size\": 1})\n\nfig.show()\n\nunder_30s_count = (df[\"duration_ms\"] &lt;= 30000).sum()\nprint(f\"Anzahl Tracks 30s oder kürzer: {under_30s_count}\")\n\nprint(\"Tracks mit Dauer über 90 Minuten:\")\nlong_tracks = df[df[\"duration_ms\"] &gt; 90 * 60 * 1000].copy()\nlong_tracks[\"duration_readable\"] = pd.to_datetime(long_tracks[\"duration_ms\"], unit=\"ms\").dt.strftime(\"%H:%M:%S\")\ndisplay(long_tracks[[\"name\", \"artists\", \"duration_readable\", \"speechiness\"]])\n\n\n                            \n                                            \n\n\nAnzahl Tracks 30s oder kürzer: 7709\nTracks mit Dauer über 90 Minuten:\n\n\n\n\n\n\n\n\n\nname\nartists\nduration_readable\nspeechiness\n\n\n\n\n4778\nDoctorow's Second Law\n['Wil Wheaton', 'Cory Doctorow']\n01:34:05\n0.9140\n\n\n4779\nDoctorow's Third Law\n['Wil Wheaton', 'Cory Doctorow']\n01:40:54\n0.8910\n\n\n11812\nBargrooves Lounge (Continuous Mix 1)\n['Various Artists']\n01:32:11\n0.0419\n\n\n149393\nGothic Lolita\n['Emilie Autumn']\n01:36:04\n0.9210\n\n\n669375\nLos Jefes - Banda Sonora de la Película (feat....\n['Cartel De Santa', 'Draw', 'Millonario', 'Mil...\n01:30:40\n0.3010\n\n\n778786\nMonstercat Podcast Ep. 086 (Staff Picks 2015)\n['Monstercat Call of the Wild']\n01:34:06\n0.1210\n\n\n877968\nBargrooves Deluxe Edition 2018 Mix 1 - Continu...\n['Various Artists']\n01:34:39\n0.0459\n\n\n877969\nBargrooves Deluxe Edition 2018 Mix 2 - Continu...\n['Various Artists']\n01:41:01\n0.0658\n\n\n885831\nBargrooves Deluxe Edition 2017 - Continuous Mix 2\n['Various Artists']\n01:35:13\n0.0557\n\n\n887120\nArc Angel - Continuous Mix\n['Planetary Assault Systems']\n01:32:57\n0.0404\n\n\n\n\n\n\n\n\n\n\nSpeechiness detektiert das Vorhandensein von gesprochenen Worten in einem Track. Wie die Verteilung zeigt, befinden sich die meisten Tracks im unteren Bereich, was für Musik typisch ist.\nEs gibt jedoch einen Anstieg bei sehr hohen Werten. Laut der Spotify Dokumentation2 handelt es sich bei Werten über 0.66 höchstwahrscheinlich um Tracks, die ausschliesslich aus gesprochenen Worten bestehen (z.B. Hörbücher, Poetry Slam, Talkshows). Da unser Fokus auf Musik liegt, werden wir Tracks mit einer Speechiness von über 0.66 aus dem Datensatz entfernen.\n\n\nCode\nfig = px.histogram(\n    x=df[\"speechiness\"], log_y=True, title=\"Verteilung der Speechiness (log)\", labels={\"x\": \"Speechiness\"}\n)\n\nfig.update_layout(yaxis_title=\"Anzahl Tracks\", xaxis_title=\"Speechiness (0-1)\", showlegend=False, yaxis={\"dtick\": 1})\nfig.update_traces(marker_line_color=\"black\", marker_line_width=1, xbins={\"start\": 0, \"end\": 1, \"size\": 0.05})\n\nfig.show()\n\nhigh_speechiness_count = (df[\"speechiness\"] &gt; 0.66).sum()\nprint(f\"Anzahl Tracks mit Speechiness &gt; 0.66: {high_speechiness_count}\")\n\n\n                            \n                                            \n\n\nAnzahl Tracks mit Speechiness &gt; 0.66: 11679\n\n\n\n\n\nBei der Analyse der Veröffentlichungsjahre stossen wir auf einige Ausreisser. Zunächst gibt es Tracks, bei denen das Jahr mit 0 angegeben ist. Diese stammen alle von einem einzigen Künstler und scheinen auf Spotify nicht mehr verfügbar zu sein. Daher werden wir diese Einträge aus unserem Datensatz entfernen.\nEin weiterer interessanter Fall sind Tracks mit dem Jahr 1900. Obwohl dies laut Wikipedia3 falsch ist, sind diese Daten so auf Spotify hinterlegt. Da wir Spotify als unsere primäre Datenquelle (“Source of Truth”) betrachten, werden wir diese Werte so belassen.\n\n\nCode\nyear_0 = df[df[\"year\"] == 0]\n\nprint(\"Tracks mit Jahr 0:\")\ndisplay(year_0[[\"name\", \"artists\", \"year\", \"release_date\"]])\n\nyear_1900 = df[df[\"year\"] == 1900]\nprint(\"Beispiele für Tracks mit Jahr 1900:\")\ndisplay(year_1900[[\"name\", \"artists\", \"year\", \"release_date\"]].head())\n\ndf_valid_years = df[df[\"year\"] &gt;= 1900]\nfig = px.histogram(\n    df_valid_years, x=\"year\", log_y=True, title=\"Verteilung der Veröffentlichungsjahre (log)\", labels={\"year\": \"Jahr\"}\n)\n\nfig.update_layout(yaxis_title=\"Anzahl Tracks\", yaxis={\"dtick\": 1})\n\nfig.show()\n\n\nTracks mit Jahr 0:\n\n\n\n\n\n\n\n\n\nname\nartists\nyear\nrelease_date\n\n\n\n\n815351\nJimmy Neutron\n['iCizzle']\n0\n0000\n\n\n815352\nI Luv You\n['iCizzle']\n0\n0000\n\n\n815353\nMy Heart\n['iCizzle']\n0\n0000\n\n\n815354\nI Am (Invincible)\n['iCizzle']\n0\n0000\n\n\n815355\nFlower Power\n['iCizzle']\n0\n0000\n\n\n815356\nHeard It Low\n['iCizzle']\n0\n0000\n\n\n815357\nHangin On\n['iCizzle']\n0\n0000\n\n\n815358\nGod Loves You\n['iCizzle']\n0\n0000\n\n\n815359\nYou In My Life\n['iCizzle']\n0\n0000\n\n\n815360\nI Wonder\n['iCizzle']\n0\n0000\n\n\n\n\n\n\n\nBeispiele für Tracks mit Jahr 1900:\n\n\n\n\n\n\n\n\n\nname\nartists\nyear\nrelease_date\n\n\n\n\n450071\nArabian Waltz\n['Rabih Abou-Khalil']\n1900\n1900-01-01\n\n\n450072\nDreams Of A Dying City\n['Rabih Abou-Khalil']\n1900\n1900-01-01\n\n\n450073\nOrnette Never Sleeps\n['Rabih Abou-Khalil']\n1900\n1900-01-01\n\n\n450074\nGeorgina\n['Rabih Abou-Khalil']\n1900\n1900-01-01\n\n\n450075\nNo Visa\n['Rabih Abou-Khalil']\n1900\n1900-01-01\n\n\n\n\n\n\n\n                            \n                                            \n\n\n\n\n\nDie Korrelationsmatrix zeigt die Zusammenhänge zwischen den verschiedenen numerischen Features.\nBesonders auffällig ist, dass die Zielvariable popularity nur sehr schwache Korrelationen mit den Audio-Features aufweist. Die stärksten Zusammenhänge bestehen zu loudness (0.15) und danceability (0.12), sowie negativ zu acousticness (-0.12) und instrumentalness (-0.12). Dies deutet darauf hin, dass die Popularität eines Songs nicht allein durch einfache Audio-Metriken erklärt werden kann und komplexere Modelle oder zusätzliche Daten (wie Songtexte) notwendig sind.\nHingegen gibt es starke Korrelationen zwischen den Audio-Features selbst, wie zum Beispiel zwischen energy und loudness (0.82) oder energy und acousticness (-0.80).\n\n\nCode\ndf_numeric = df.select_dtypes(include=\"number\")\ndf_numeric = df_numeric.drop(columns=[\"track_number\", \"disc_number\"], errors=\"ignore\")\n\ncorr_matrix = df_numeric.corr().round(2)\n\nfig = px.imshow(\n    corr_matrix,\n    text_auto=True,\n    aspect=\"auto\",\n    color_continuous_scale=\"RdBu_r\",\n    zmin=-1,\n    zmax=1,\n    title=\"Korrelationsmatrix\",\n)\n\nfig.update_traces(hovertemplate=\"Feature 1: %{x}&lt;br&gt;Feature 2: %{y}&lt;br&gt;Korrelation: %{z}&lt;extra&gt;&lt;/extra&gt;\")\nfig.update_layout(xaxis_title=\"Features\", yaxis_title=\"Features\")\n\nfig.show()\n\n\n                            \n                                            \n\n\nLoudness vs Popularity\nWie in der Korrelationsmatrix ersichtlich, ist loudness eines der wenigen Features, das eine nennenswerte positive Korrelation mit popularity aufweist. Dies spiegelt das Phänomen des “Loudness War”4 wider, bei dem Musikproduzenten dazu neigen, Tracks lauter zu mastern, um im Radio oder in Playlists mehr Aufmerksamkeit zu erregen und subjektiv “besser” zu klingen.\n\n\n\nZusammenfassend lässt sich sagen, dass die Datenqualität des Spotify-Datensatzes technisch sehr hoch ist. Es gibt keine fehlenden Werte (nach Korrektur der “None”-Strings) und keine Duplikate. Die Wertebereiche der Audio-Features sind konsistent und gut dokumentiert.\nAllerdings zeigt die Analyse der Popularitätsverteilung, dass ein sehr grosser Teil der Daten (Tracks mit Popularität 0) für unsere Zielsetzung - die Vorhersage von Song-Popularität - irrelevant ist. Diese “toten” Datenpunkte würden ein Modell eher verwirren als trainieren, weshalb wir im weiteren Verlauf eine aggressive Filterung vornehmen werden, um uns auf relevante Musik-Tracks zu konzentrieren.\nZudem haben wir festgestellt, dass die Korrelationen zwischen den einfachen Audio-Features und der Popularität generell schwach sind. Dies ist eine wichtige Erkenntnis: Der Erfolg eines Songs lässt sich nicht allein durch Metriken wie Tempo, Tonart oder Tanzbarkeit erklären. Dies bestätigt unsere Hypothese, dass wir komplexere Merkmale benötigen, weshalb wir mit dem nächsten Dataset Songtexte als zusätzliche Datenquelle hinzuziehen.\n\n\n\n\n\nDa wir beim Modellieren bemerkt haben, dass wir mehr Daten für bessere Modellperformance brauchen haben wir uns entschieden, die Songtexte der Tracks anzuschaffen, falls vorhanden.\nUm dies zu erreichen haben wir uns für das massive “Genius Song Lyrics” Dataset von CarlosGDCJ auf kaggle.com entschieden. Es enthält eine 9.07 GB grosse CSV Datei, welche Songtexte für über 7 Millionen Songs von der Webseite genius.com beinhaltet. Auf der einen Seite mussten wir somit nicht selber die Daten scrapen und hatten einen Grossteil der Texte für unser tracks_features.csv dataset. Auf der anderen Seite sind Spotify und Genius verschiedene Dienste, was bedeutet wir haben keinen eindeutigen Identifikator um die zwei Datasets zu joinen. Diese Problematik wird später im Kapitel Prozessierte Daten verdeutlicht.\n\n\nDer verwendete Datensatz weist keine explizite eigene Lizenz aus. Laut Beschreibung des Autors wurden die enthaltenen Daten von genius.com gescraped.\n\nFair Use: Die Nutzung erfolgt ausschliesslich zu nicht-kommerziellen, schulischen Analysezwecken; es werden keine vollständigen Songtexte veröffentlicht.\nDatenschutz: Der Datensatz wird ausschliesslich projektintern gespeichert und technisch so gesichert, dass kein unautorisierter Zugriff durch Dritte möglich ist.\nUrheberrecht: Die Texte werden nur zu analytischen Zwecken verarbeitet und nicht weiterverbreitet oder kommerziell genutzt.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndex\nName\nDatentyp\nWerte\nBeschreibung\n\n\n\n\n1\ntitle\nstring\n-\nTitel des Stücks. Meistens Songs, aber auch Bücher, Gedichte etc.\n\n\n2\ntag\nstring\n-\nGenre des Stücks. Meistens “pop”, “rap”, “rock”, “rb”, “country” oder “misc”\n\n\n3\nartist\nstring\n-\nKünstler oder Gruppe, dem das Stück zugeschrieben wird\n\n\n4\nyear\ninteger\nFormat: YYYY\nVeröffentlichungsjahr\n\n\n5\nviews\ninteger\n-\nAnzahl der Seitenaufrufe auf Genius\n\n\n6\nfeatures\nListe von strings\n-\nAlle Künstler, die beigetragen haben\n\n\n7\nlyrics\nstring\n-\nDer Songtext\n\n\n8\nid\ninteger\n-\nGenius Identifier\n\n\n9\nlanguage_cld3\nstring\nISO 639-1 Codes\nSprache des Songtextes laut CLD3\n\n\n10\nlanguage_ft\nstring\nISO 639-1 Codes\nSprache des Songtextes laut FastText\n\n\n11\nlanguage\nstring\nISO 639-1 Codes\nKombinierte Sprache (nur wenn beide Modelle übereinstimmen)\n\n\n\n(Für zusätzliche Informationen siehe Genius Song Lyrics Dataset auf Kaggle)\n\n\n\nDiese Datenanalyse wurde in Python 3.14 mit den pandas und plotly Paketen durchgeführt. Mehr Details können auf dem Repository im pyproject.toml gefunden werden. Diese Datenexploration wurde recht kurz gehalten, da wir nur die Songtexte von diesem Dataset brauchen.\n\nÜbersicht Datenqualität\n\n\nAnzahl Spalten\n11\n\n\nAnzahl Zeilen\n5134856\n\n\nAnzahl leerer Zellen\n452394\n\n\nAnteil (%) leerer Zellen\n0.8%\n\n\nAnzahl duplizierter Zeilen\n0\n\n\nAnteil (%) duplizierter Zeilen\n0%\n\n\n\n\n\nDie Spalte lyrics enthält die Songtexte. Auffällig sind hierbei die Sektions-Tags in eckigen Klammern, wie zum Beispiel [Chorus], [Verse] oder [Intro]. Diese dienen auf Genius der Strukturierung, sind jedoch kein Teil des gesungenen Textes. Um die Qualität der Textanalyse (Embeddings) zu verbessern, werden wir diese Tags im Prozessierungsschritt entfernen.\n\n\nCode\nfrom pathlib import Path\n\nimport pandas as pd\n\nGENIUS_PATH = Path(\"../data/song_lyrics.csv\")\ndf_genius = pd.read_csv(GENIUS_PATH)\n\nfirst_row = df_genius.head(1)[[\"title\", \"features\", \"lyrics\"]].copy()\nfirst_row[\"lyrics\"] = first_row[\"lyrics\"].str[:79]\ndisplay(first_row)\n\n\n\n\n\n\n\n\n\ntitle\nfeatures\nlyrics\n\n\n\n\n0\nKilla Cam\n{\"Cam\\\\'ron\",\"Opera Steve\"}\n[Chorus: Opera Steve & Cam'ron]\\nKilla Cam, Ki...\n\n\n\n\n\n\n\n\n\n\nDie artist Spalte im Genius Datensatz weist Formatierungsprobleme auf, bei denen Sonderzeichen (wie Umlaute) oft einfach weggelassen werden. Wie im folgenden Code-Beispiel ersichtlich, wird “Motörhead” zu “Motrhead” und “Blue Öyster Cult” zu “Blue yster Cult”. Interessanterweise enthält die features Spalte, welche eine Liste der beteiligten Künstler beinhaltet, oft die korrekte Schreibweise. Diese Erkenntnis ist entscheidend für das spätere Zusammenführen mit dem Spotify-Datensatz.\n\n\nCode\nfrom IPython.display import display\n\nsong_motorhead = df_genius[df_genius[\"artist\"] == \"Motrhead\"][[\"title\", \"artist\", \"features\"]]\nsong_byc = df_genius[df_genius[\"artist\"] == \"Blue yster Cult\"][[\"title\", \"artist\", \"features\"]]\ndisplay(song_motorhead.head())\ndisplay(song_byc.head())\n\n\n\n\n\n\n\n\n\ntitle\nartist\nfeatures\n\n\n\n\n50811\nAce of Spades\nMotrhead\n{Motörhead}\n\n\n54677\nOrgasmatron\nMotrhead\n{Motörhead}\n\n\n81582\nWe Are The Road Crew\nMotrhead\n{Motörhead}\n\n\n115348\nFire Fire\nMotrhead\n{Motörhead}\n\n\n121838\nIts a Long Way to the Top If You Wanna Rock N ...\nMotrhead\n{Motörhead}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntitle\nartist\nfeatures\n\n\n\n\n123021\nDont Fear The Reaper\nBlue yster Cult\n{\"Blue Öyster Cult\"}\n\n\n140918\nCareer of Evil\nBlue yster Cult\n{\"Patti Smith\",\"Blue Öyster Cult\"}\n\n\n198572\nGodzilla\nBlue yster Cult\n{\"Blue Öyster Cult\"}\n\n\n224036\nAstronomy\nBlue yster Cult\n{\"Blue Öyster Cult\"}\n\n\n234153\nBurnin’ for You\nBlue yster Cult\n{\"Blue Öyster Cult\"}\n\n\n\n\n\n\n\n\nEin weiteres Problem ist die Inkonsistenz bei der Benennung von Künstlern. Wie das Beispiel des Songs “Maneater” zeigt, werden die Künstler im Genius-Datensatz als “Hall & Oates” geführt, während sie im Spotify-Datensatz als “Daryl Hall & John Oates” auftreten. Diese Diskrepanzen erschweren einen direkten Join über den Künstlernamen und bedeuteten, dass wir einige Songs nicht matchen werden können.\n\n\nCode\ndf_genius_maneater = df_genius[(df_genius[\"title\"] == \"Maneater\") & (df_genius[\"year\"] == 1982)][\n    [\"title\", \"artist\", \"features\", \"year\"]\n]\n\nprint(\"Maneater in Genius dataset:\")\ndisplay(df_genius_maneater)\n\ndf_spotify_maneater = df[(df[\"name\"] == \"Maneater\") & (df[\"year\"] == 1982)][[\"name\", \"artists\", \"year\"]]\n\nprint(\"Maneater in Spotify dataset:\")\ndisplay(df_spotify_maneater)\n\n\nManeater in Genius dataset:\n\n\n\n\n\n\n\n\n\ntitle\nartist\nfeatures\nyear\n\n\n\n\n276117\nManeater\nHall & Oates\n{}\n1982\n\n\n\n\n\n\n\nManeater in Spotify dataset:\n\n\n\n\n\n\n\n\n\nname\nartists\nyear\n\n\n\n\n604861\nManeater\n['Daryl Hall & John Oates']\n1982\n\n\n\n\n\n\n\n\n\n\nObwohl wir primär nur an der lyrics Spalte interessiert sind, sind die Spalten title, artist und features essentiell für das Zusammenführen mit dem Spotify-Datensatz. Wie oben beschrieben, weisen genau diese Spalten Inkonsistenzen und Formatierungsprobleme auf (fehlende Sonderzeichen, abweichende Schreibweisen). Dies erforderte zusätzlichen Aufwand beim Daten-Matching, um eine möglichst hohe Trefferquote zu erzielen und sicherzustellen, dass wir so viele Songtexte wie möglich den korrekten Spotify-Tracks zuordnen können."
  },
  {
    "objectID": "2_datenbericht.html#details-spotify-songs-kaggle-api",
    "href": "2_datenbericht.html#details-spotify-songs-kaggle-api",
    "title": "Datenbericht",
    "section": "",
    "text": "Die Grundbasis dieser Arbeit ist das “Spotify 1.2M+ Songs” Dataset von Rodolfo Figueroa auf kaggle.com. Enthalten ist eine CSV-Datei, tracks_features.csv, welche Daten zu über 1.2 Millionen Songs auf Spotify von der Spotify Developer API enthalten. Spezifisch von den /tracks und /audio-features endpoints.\nAudio Features sind Informationen über ein Lied, welche zum einen direkt dem Lied entnommen wurden, wie z.B. duration_ms, tempo und key, sowohl auch Werte welche durch eine Analyse von Spotify kalkuliert wurden (z.B. speechiness, valence und energy).\nLeider fehlt in diesem Dataset eine wichtige Kennzahl, popularity. Deswegen haben wir diese Zahlen selber anhand der Songs im Dataset von der API entnommen.\nMithilfe eines API-Accounts und dem folgenden Python Skript haben wir dies erreicht. Es..\n\nliest alle Spotify Track IDs des tracks_features.csv ein\nfragt den API endpoint tracks in Batches von 50 Tracks ab\nspeichert die Responses pro Batch im directory data/raw_responses als batch_{i}.json ab.\n\n\nimport json\nimport math\nimport os\nimport time\nfrom pathlib import Path\n\nimport pandas as pd\nimport requests as rq\nfrom dotenv import load_dotenv\nfrom tqdm import tqdm\n\nload_dotenv()\n\nCLIENT_ID = os.getenv(\"SPOTIFY_CLIENT_ID\")\nCLIENT_SECRET = os.getenv(\"SPOTIFY_CLIENT_SECRET\")\nOUTPUT_DIR = Path(\"data/raw_responses\")\nBATCH_SIZE = 50\n\n\ndef get_access_token() -&gt; str:\n    auth_url = \"https://accounts.spotify.com/api/token\"\n    try:\n        auth_response = rq.post(\n            auth_url, {\"grant_type\": \"client_credentials\", \"client_id\": CLIENT_ID, \"client_secret\": CLIENT_SECRET}\n        )\n        auth_response.raise_for_status()\n        return auth_response.json()[\"access_token\"]\n    except Exception as e:\n        print(f\"Failed to get access token: {e}\")\n        raise\n\n\ndef main() -&gt; None:\n    df = pd.read_csv(\"data/tracks_features.csv\")\n    track_ids = df[\"id\"].dropna().unique().tolist()\n\n    total_tracks = len(track_ids)\n    print(f\"Found {total_tracks} tracks.\")\n\n    token = get_access_token()\n    headers = {\"Authorization\": f\"Bearer {token}\"}\n\n    num_batches = math.ceil(total_tracks / BATCH_SIZE)\n\n    for i in tqdm(range(num_batches), desc=\"Fetching batches\"):\n        batch_file = OUTPUT_DIR / f\"batch_{i}.json\"\n\n        # Skip if already processed\n        if batch_file.exists():\n            continue\n\n        start_idx = i * BATCH_SIZE\n        end_idx = start_idx + BATCH_SIZE\n        batch_ids = track_ids[start_idx:end_idx]\n\n        ids_param = \",\".join(batch_ids)\n        url = f\"https://api.spotify.com/v1/tracks?ids={ids_param}\"\n\n        while True:\n            try:\n                time.sleep(2)\n                response = rq.get(url, headers=headers)\n\n                if response.status_code == 200:\n                    data = response.json()\n                    with batch_file.open(\"w\") as f:\n                        json.dump(data, f)\n                    break\n\n                if response.status_code == 429:\n                    retry_after = int(response.headers.get(\"Retry-After\", 5))\n                    tqdm.write(f\"Rate limited. Sleeping for {retry_after} seconds.\")\n                    time.sleep(retry_after + 1)\n\n                elif response.status_code == 401:\n                    tqdm.write(\"Token expired. Refreshing...\")\n                    token = get_access_token()\n                    headers = {\"Authorization\": f\"Bearer {token}\"}\n\n                else:\n                    tqdm.write(f\"Error {response.status_code} for batch {i}: {response.text}\")\n                    # Log error and skip this batch to avoid blocking\n                    error_file = OUTPUT_DIR / f\"error_batch_{i}.txt\"\n                    with error_file.open(\"w\") as f:\n                        f.write(f\"Status: {response.status_code}\\n{response.text}\")\n                    break\n\n            except rq.exceptions.RequestException as e:\n                tqdm.write(f\"Request exception: {e}\")\n                time.sleep(5)\n\n\nif __name__ == \"__main__\":\n    main()\n\nDanach hatten wir 24’081 JSON files, die wir mit dem tracks_features.csv Dataset joinen mussten. Dies haben wir mit dem folgenden Skript vollendet. Aus Performance Gründen haben wir das orjson Python package und eingebautes Multiprocessing verwendet:\n\nfrom concurrent.futures import ThreadPoolExecutor\nfrom pathlib import Path\n\nimport orjson\n\n# load all json files in data/raw_responses and combine into a single dataframe\ndata_path = Path(\"../data/raw_responses\")\njson_files = list(data_path.glob(\"*.json\"))\nprint(f\"Found {len(json_files)} JSON files.\")\n\n\ndef process_file(file_path: Path) -&gt; list:\n    data = orjson.loads(file_path.open(\"rb\").read())\n    tracks = []\n    for t in data.get(\"tracks\", []):\n        t.pop(\"available_markets\", None)\n        tracks.append(t)\n    return tracks\n\n\nprint(\"Processing files in parallel...\")\ntracks_all = []\nwith ThreadPoolExecutor() as executor:\n    results = executor.map(process_file, json_files)\n\n    for tracks in tqdm(results, total=len(json_files)):\n        tracks_all.extend(tracks)\n\ndf = pd.DataFrame(tracks_all)\nprint(f\"Total tracks loaded: {len(df)}\")\n\nFound 24081 JSON files.\nProcessing files in parallel...\n\n\n100%|██████████| 24081/24081 [02:19&lt;00:00, 172.81it/s]\n\n\nTotal tracks loaded: 1204025\n\n\n\ndf_features = pd.read_csv(\"../data/tracks_features.csv\", keep_default_na=False)\nprint(f\"Tracks geladen: {len(df_features)}\")\n\ndf_combined = df_features.merge(df[[\"id\", \"popularity\"]], on=\"id\", how=\"left\")\nprint(f\"Kombinierte shape: {df_combined.shape}\")\n\n# How many rows with missing popularity?\nmissing_popularity = df_combined[\"popularity\"].isna().sum()\nprint(f\"Zeilen mit fehlender popularity: {missing_popularity}\")\n\n# Export to CSV\noutput_path = Path(\"../data/full_tracks_features.csv\")\ndf_combined.to_csv(output_path, index=False)\n\nTracks geladen: 1204025\nKombinierte shape: (1204025, 25)\nZeilen mit fehlender popularity: 0\n\n\nNun hatten wir die popularity Metrik für alle 1’204’025 Tracks hinzugefügt und als eine neue Datei full_tracks_features.csv abgepeichert.\n\n\n\nKaggle-Dataset: Der verwendete Datensatz weist keine explizite eigene Lizenz aus. Laut Beschreibung des Autors wurden die enthaltenen Daten über die Spotify Web API erhoben.\nSpotify API: Die Nutzung der Spotify Web API unterliegt den Spotify Developer Terms (Version 10, gültig seit 15. Mai 2025).\n\nFair Use: Der Zugriff auf die Spotify API erfolgt unter Einhaltung der vorgegebenen Rate Limits. Die API-Zugangsdaten werden sicher verwaltet und nicht öffentlich zugänglich gemacht.\nDatenschutz: Es werden keine personenbezogenen Daten von Spotify-Nutzern verarbeitet.\nUrheberrecht: Es werden keine urheberrechtlich geschützte Inhalte bezogen.\nMachine-Learning: Die eingesetzten Machine-Learning-Verfahren dienen ausschliesslich der statistischen Modellierung und Analyse dieser Metadaten im Rahmen eines Nicht-kommerziellen Schulprojekts.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndex\nName\nDatentyp\nWerte\nBeschreibung\n\n\n\n\n1\nid\nstring\nFormat: base-62\nDie Spotify-ID für den Track\n\n\n2\nname\nstring\n-\nDer Name des Tracks\n\n\n3\nalbum\nstring\n-\nDas Album, auf dem der Track erscheint\n\n\n4\nalbum_id\nstring\nFormat: base-62\nDie Spotify-ID für das Album\n\n\n5\nartists\nListe von strings\n-\nDie Künstler, die den Track performt haben\n\n\n6\nartist_id\nListe von strings\nFormat: base-62\nDie Spotify-ID für die Künstler\n\n\n7\ntrack_number\ninteger\nWertebereich: 1 - 50\nDie Nummer des Tracks auf dem Album\n\n\n8\ndisc_number\ninteger\nWertebereich: 1 - 13\nDie Disc-Nummer, auf dem der Track erscheint\n\n\n9\nexplicit\nboolean\ntrue = Jafalse = Nein oder unbekannt\nOb der Track explizite Texte enthält\n\n\n10\ndanceability\nfloat\nWertebereich: 0 - 1\nTanzbarkeit beschreibt, wie geeignet ein Track zum Tanzen ist, basierend auf einer Kombination musikalischer Elemente.0.0 → am wenigsten tanzbar, 1.0 → am tanzbarsten\n\n\n11\nenergy\nfloat\nWertebereich: 0 - 1\nWahrnehmungsmass für Intensität und Aktivität dar, typischerweise fühlen sich energiegeladene Tracks schnell, laut und geräuschvoll an\n\n\n12\nkey\ninteger\nWertebereich: -1 - 11\nDie Tonart, in der sich der Track befindet, basierend auf Standard-Pitch-Class-Notation, Wert -1 = keine Tonart erkannt\n\n\n13\nloudness\nfloat\nWertebereich: -60 - 0Einheit: Dezibel (dB)\nDie Gesamtlautstärke eines Tracks in Dezibel (dB)\n\n\n14\nmode\ninteger\n1 = Major, 0 = Minor\nGibt die Tonalität (Dur oder Moll) eines Tracks an\n\n\n15\nspeechiness\nfloat\nWertebereich: 0 - 1\nErkennt das Vorhandensein von gesprochenen Worten in einem Track\n\n\n16\nacousticness\nfloat\nWertebereich: 0 - 1\nKonfidenzmass ob der Track akustisch ist\n\n\n17\ninstrumentalness\nfloat\nWertebereich: 0 - 1\nSagt voraus, ob ein Track keinen Gesang enthält\n\n\n18\nliveness\nfloat\nWertebereich: 0 - 1\nErkennt die Anwesenheit eines Publikums in der Aufnahme\n\n\n19\nvalence\nfloat\nWertebereich: 0 - 1\nBeschreibt die musikalische Positivität, die von einem Track vermittelt wird\n\n\n20\ntempo\nfloat\nEinheit: beats per minute (BPM)\nDas geschätzte Gesamttempo eines Tracks in Schlägen pro Minute (BPM)\n\n\n21\nduration_ms\nfloat\nEinheit: Millisekunden (ms)\nDie Dauer des Tracks in Millisekunden.\n\n\n22\ntime_signature\ninteger\nWertebereich: 3 - 7\nEine geschätzte Taktart, gibt wie viele Schläge in jedem Takt enthalten sind\n\n\n23\nyear\ninteger\nFormat: YYYY\nDas Jahr des Veröffentlichungsdatums des Tracks\n\n\n24\nrelease_date\nstring\nFormate:YYYY,YYYY-MM,YYYY-MM-DD\nDas Datum, an dem das Album erstmals veröffentlicht wurde. Präzision und somit Format variiert\n\n\n25\npopularity\ninteger\nWertebereich: 0 - 100\nDie Popularität des Tracks, basiert haupstächlich auf Gesamtzahl der Wiedergaben\n\n\n\n(Für zusätzliche Informationen siehe Spotify API Dokumentation)\n\n\n\nDiese Datenanalyse wurde in Python 3.14 mit den pandas und plotly Paketen durchgeführt. Mehr Details können auf dem Repository im pyproject.toml gefunden werden.\n\nÜbersicht Datenqualität\n\n\nAnzahl Spalten\n25\n\n\nAnzahl Zeilen\n1’204’025\n\n\nAnzahl leerer Zellen\n0\n\n\nAnteil (%) leerer Zellen\n0%\n\n\nAnzahl duplizierter Zeilen\n0\n\n\nAnteil (%) duplizierter Zeilen\n0%\n\n\n\n\n\nDa die Strings im CSV nicht wrapped sind mit Anführungszeichen, hat Pandas die Datei am Anfang der Analyse falsch geladen. Denn es gibt ein Album vom Künstler Gupi mit dem Namen “None”. Pandas hat dies standardmässig als einen fehlenden Wert interpretiert. Um dies zu lösen haben wir für die Analyse und das Weiterverarbeiten von nun an die Datei folgendermassen geladen:\ndf = pd.read_csv(Path(\"../data/full_tracks_features.csv\"), keep_default_na=False, na_values=[\"\"])\n\n\n\nÜberaschenderweise gibt es keine duplizierte Zeilen, obwohl (wie die Dokumentation1 erwähnt) die id Spalte einen Track nicht eindeutig identifiziert.\n\n\n\nDie Verteilung der Popularität ist extrem rechtsschief. Wie die folgende Grafik zeigt, hat ein massiver Anteil der Tracks im Datensatz eine Popularität von 0. Da die Anzahl der Tracks mit Popularität 0 die restlichen Werte bei weitem übersteigt, wird hier eine logarithmische Skala für die y-Achse verwendet.\nDiese grosse Menge an Tracks mit dem Wert 0 deutet auf viele inaktive, sehr alte oder extrem nischenhafte Songs hin, die auf der Plattform kaum oder gar nicht gehört werden. Für die spätere Modellierung ist dies ein entscheidender Faktor, da wir entscheiden müssen, ob wir diese “toten” Datenpunkte behalten oder filtern wollen.\n\n\nCode\nfrom pathlib import Path\n\nimport pandas as pd\nimport plotly.express as px\nimport plotly.io as pio\n\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\n\ndf = pd.read_csv(Path(\"../data/full_tracks_features.csv\"), keep_default_na=False, na_values=[\"\"])\n\nfig = px.histogram(x=df[\"popularity\"], log_y=True, title=\"Verteilung der Popularität (log)\", labels={\"x\": \"Popularity\"})\n\nfig.update_layout(yaxis_title=\"Anzahl Tracks\", xaxis_title=\"Popularity (0-100)\", showlegend=False, yaxis={\"dtick\": 1})\nfig.update_traces(marker_line_color=\"black\", marker_line_width=1)\n\nfig.show()\n\npop_0_count = (df[\"popularity\"] == 0).sum()\npop_gt_0_count = (df[\"popularity\"] &gt; 0).sum()\nprint(f\"Anzahl Tracks mit popularity 0:     {pop_0_count}\")\nprint(f\"Anzahl Tracks mit popularity &gt; 0:   {pop_gt_0_count}\")\n\n\n                            \n                                            \n\n\nAnzahl Tracks mit popularity 0:     729167\nAnzahl Tracks mit popularity &gt; 0:   474858\n\n\n\n\n\nDie Verteilung der Dauer der Tracks zeigt ebenfalls interessante Muster. Auch hier verwenden wir eine logarithmische Skala für die y-Achse.\nEs gibt eine Anzahl an Tracks, die sehr kurz sind (unter 30 Sekunden), konkret 7’709 Stück. Dies sind oft Soundeffekte, Intros oder Interludes. Obwohl dies im Vergleich zur Gesamtmenge ein kleiner Anteil ist, filtern wir diese später heraus, um die Datenqualität zu erhöhen. Zusätzlich zeigt die Tabelle unterhalb der Grafik einige Extremwerte am anderen Ende des Spektrums: Tracks mit einer Dauer von über 90 Minuten.\n\n\nCode\nfig = px.histogram(\n    x=df[\"duration_ms\"] / 1000 / 60, log_y=True, title=\"Verteilung der Dauer (log)\", labels={\"x\": \"Dauer (min)\"}\n)\n\nfig.update_layout(yaxis_title=\"Anzahl Tracks\", xaxis_title=\"Dauer (min)\", showlegend=False, yaxis={\"dtick\": 1})\nfig.update_traces(marker_line_color=\"black\", marker_line_width=1, xbins={\"size\": 1})\n\nfig.show()\n\nunder_30s_count = (df[\"duration_ms\"] &lt;= 30000).sum()\nprint(f\"Anzahl Tracks 30s oder kürzer: {under_30s_count}\")\n\nprint(\"Tracks mit Dauer über 90 Minuten:\")\nlong_tracks = df[df[\"duration_ms\"] &gt; 90 * 60 * 1000].copy()\nlong_tracks[\"duration_readable\"] = pd.to_datetime(long_tracks[\"duration_ms\"], unit=\"ms\").dt.strftime(\"%H:%M:%S\")\ndisplay(long_tracks[[\"name\", \"artists\", \"duration_readable\", \"speechiness\"]])\n\n\n                            \n                                            \n\n\nAnzahl Tracks 30s oder kürzer: 7709\nTracks mit Dauer über 90 Minuten:\n\n\n\n\n\n\n\n\n\nname\nartists\nduration_readable\nspeechiness\n\n\n\n\n4778\nDoctorow's Second Law\n['Wil Wheaton', 'Cory Doctorow']\n01:34:05\n0.9140\n\n\n4779\nDoctorow's Third Law\n['Wil Wheaton', 'Cory Doctorow']\n01:40:54\n0.8910\n\n\n11812\nBargrooves Lounge (Continuous Mix 1)\n['Various Artists']\n01:32:11\n0.0419\n\n\n149393\nGothic Lolita\n['Emilie Autumn']\n01:36:04\n0.9210\n\n\n669375\nLos Jefes - Banda Sonora de la Película (feat....\n['Cartel De Santa', 'Draw', 'Millonario', 'Mil...\n01:30:40\n0.3010\n\n\n778786\nMonstercat Podcast Ep. 086 (Staff Picks 2015)\n['Monstercat Call of the Wild']\n01:34:06\n0.1210\n\n\n877968\nBargrooves Deluxe Edition 2018 Mix 1 - Continu...\n['Various Artists']\n01:34:39\n0.0459\n\n\n877969\nBargrooves Deluxe Edition 2018 Mix 2 - Continu...\n['Various Artists']\n01:41:01\n0.0658\n\n\n885831\nBargrooves Deluxe Edition 2017 - Continuous Mix 2\n['Various Artists']\n01:35:13\n0.0557\n\n\n887120\nArc Angel - Continuous Mix\n['Planetary Assault Systems']\n01:32:57\n0.0404\n\n\n\n\n\n\n\n\n\n\nSpeechiness detektiert das Vorhandensein von gesprochenen Worten in einem Track. Wie die Verteilung zeigt, befinden sich die meisten Tracks im unteren Bereich, was für Musik typisch ist.\nEs gibt jedoch einen Anstieg bei sehr hohen Werten. Laut der Spotify Dokumentation2 handelt es sich bei Werten über 0.66 höchstwahrscheinlich um Tracks, die ausschliesslich aus gesprochenen Worten bestehen (z.B. Hörbücher, Poetry Slam, Talkshows). Da unser Fokus auf Musik liegt, werden wir Tracks mit einer Speechiness von über 0.66 aus dem Datensatz entfernen.\n\n\nCode\nfig = px.histogram(\n    x=df[\"speechiness\"], log_y=True, title=\"Verteilung der Speechiness (log)\", labels={\"x\": \"Speechiness\"}\n)\n\nfig.update_layout(yaxis_title=\"Anzahl Tracks\", xaxis_title=\"Speechiness (0-1)\", showlegend=False, yaxis={\"dtick\": 1})\nfig.update_traces(marker_line_color=\"black\", marker_line_width=1, xbins={\"start\": 0, \"end\": 1, \"size\": 0.05})\n\nfig.show()\n\nhigh_speechiness_count = (df[\"speechiness\"] &gt; 0.66).sum()\nprint(f\"Anzahl Tracks mit Speechiness &gt; 0.66: {high_speechiness_count}\")\n\n\n                            \n                                            \n\n\nAnzahl Tracks mit Speechiness &gt; 0.66: 11679\n\n\n\n\n\nBei der Analyse der Veröffentlichungsjahre stossen wir auf einige Ausreisser. Zunächst gibt es Tracks, bei denen das Jahr mit 0 angegeben ist. Diese stammen alle von einem einzigen Künstler und scheinen auf Spotify nicht mehr verfügbar zu sein. Daher werden wir diese Einträge aus unserem Datensatz entfernen.\nEin weiterer interessanter Fall sind Tracks mit dem Jahr 1900. Obwohl dies laut Wikipedia3 falsch ist, sind diese Daten so auf Spotify hinterlegt. Da wir Spotify als unsere primäre Datenquelle (“Source of Truth”) betrachten, werden wir diese Werte so belassen.\n\n\nCode\nyear_0 = df[df[\"year\"] == 0]\n\nprint(\"Tracks mit Jahr 0:\")\ndisplay(year_0[[\"name\", \"artists\", \"year\", \"release_date\"]])\n\nyear_1900 = df[df[\"year\"] == 1900]\nprint(\"Beispiele für Tracks mit Jahr 1900:\")\ndisplay(year_1900[[\"name\", \"artists\", \"year\", \"release_date\"]].head())\n\ndf_valid_years = df[df[\"year\"] &gt;= 1900]\nfig = px.histogram(\n    df_valid_years, x=\"year\", log_y=True, title=\"Verteilung der Veröffentlichungsjahre (log)\", labels={\"year\": \"Jahr\"}\n)\n\nfig.update_layout(yaxis_title=\"Anzahl Tracks\", yaxis={\"dtick\": 1})\n\nfig.show()\n\n\nTracks mit Jahr 0:\n\n\n\n\n\n\n\n\n\nname\nartists\nyear\nrelease_date\n\n\n\n\n815351\nJimmy Neutron\n['iCizzle']\n0\n0000\n\n\n815352\nI Luv You\n['iCizzle']\n0\n0000\n\n\n815353\nMy Heart\n['iCizzle']\n0\n0000\n\n\n815354\nI Am (Invincible)\n['iCizzle']\n0\n0000\n\n\n815355\nFlower Power\n['iCizzle']\n0\n0000\n\n\n815356\nHeard It Low\n['iCizzle']\n0\n0000\n\n\n815357\nHangin On\n['iCizzle']\n0\n0000\n\n\n815358\nGod Loves You\n['iCizzle']\n0\n0000\n\n\n815359\nYou In My Life\n['iCizzle']\n0\n0000\n\n\n815360\nI Wonder\n['iCizzle']\n0\n0000\n\n\n\n\n\n\n\nBeispiele für Tracks mit Jahr 1900:\n\n\n\n\n\n\n\n\n\nname\nartists\nyear\nrelease_date\n\n\n\n\n450071\nArabian Waltz\n['Rabih Abou-Khalil']\n1900\n1900-01-01\n\n\n450072\nDreams Of A Dying City\n['Rabih Abou-Khalil']\n1900\n1900-01-01\n\n\n450073\nOrnette Never Sleeps\n['Rabih Abou-Khalil']\n1900\n1900-01-01\n\n\n450074\nGeorgina\n['Rabih Abou-Khalil']\n1900\n1900-01-01\n\n\n450075\nNo Visa\n['Rabih Abou-Khalil']\n1900\n1900-01-01\n\n\n\n\n\n\n\n                            \n                                            \n\n\n\n\n\nDie Korrelationsmatrix zeigt die Zusammenhänge zwischen den verschiedenen numerischen Features.\nBesonders auffällig ist, dass die Zielvariable popularity nur sehr schwache Korrelationen mit den Audio-Features aufweist. Die stärksten Zusammenhänge bestehen zu loudness (0.15) und danceability (0.12), sowie negativ zu acousticness (-0.12) und instrumentalness (-0.12). Dies deutet darauf hin, dass die Popularität eines Songs nicht allein durch einfache Audio-Metriken erklärt werden kann und komplexere Modelle oder zusätzliche Daten (wie Songtexte) notwendig sind.\nHingegen gibt es starke Korrelationen zwischen den Audio-Features selbst, wie zum Beispiel zwischen energy und loudness (0.82) oder energy und acousticness (-0.80).\n\n\nCode\ndf_numeric = df.select_dtypes(include=\"number\")\ndf_numeric = df_numeric.drop(columns=[\"track_number\", \"disc_number\"], errors=\"ignore\")\n\ncorr_matrix = df_numeric.corr().round(2)\n\nfig = px.imshow(\n    corr_matrix,\n    text_auto=True,\n    aspect=\"auto\",\n    color_continuous_scale=\"RdBu_r\",\n    zmin=-1,\n    zmax=1,\n    title=\"Korrelationsmatrix\",\n)\n\nfig.update_traces(hovertemplate=\"Feature 1: %{x}&lt;br&gt;Feature 2: %{y}&lt;br&gt;Korrelation: %{z}&lt;extra&gt;&lt;/extra&gt;\")\nfig.update_layout(xaxis_title=\"Features\", yaxis_title=\"Features\")\n\nfig.show()\n\n\n                            \n                                            \n\n\nLoudness vs Popularity\nWie in der Korrelationsmatrix ersichtlich, ist loudness eines der wenigen Features, das eine nennenswerte positive Korrelation mit popularity aufweist. Dies spiegelt das Phänomen des “Loudness War”4 wider, bei dem Musikproduzenten dazu neigen, Tracks lauter zu mastern, um im Radio oder in Playlists mehr Aufmerksamkeit zu erregen und subjektiv “besser” zu klingen.\n\n\n\nZusammenfassend lässt sich sagen, dass die Datenqualität des Spotify-Datensatzes technisch sehr hoch ist. Es gibt keine fehlenden Werte (nach Korrektur der “None”-Strings) und keine Duplikate. Die Wertebereiche der Audio-Features sind konsistent und gut dokumentiert.\nAllerdings zeigt die Analyse der Popularitätsverteilung, dass ein sehr grosser Teil der Daten (Tracks mit Popularität 0) für unsere Zielsetzung - die Vorhersage von Song-Popularität - irrelevant ist. Diese “toten” Datenpunkte würden ein Modell eher verwirren als trainieren, weshalb wir im weiteren Verlauf eine aggressive Filterung vornehmen werden, um uns auf relevante Musik-Tracks zu konzentrieren.\nZudem haben wir festgestellt, dass die Korrelationen zwischen den einfachen Audio-Features und der Popularität generell schwach sind. Dies ist eine wichtige Erkenntnis: Der Erfolg eines Songs lässt sich nicht allein durch Metriken wie Tempo, Tonart oder Tanzbarkeit erklären. Dies bestätigt unsere Hypothese, dass wir komplexere Merkmale benötigen, weshalb wir mit dem nächsten Dataset Songtexte als zusätzliche Datenquelle hinzuziehen."
  },
  {
    "objectID": "2_datenbericht.html#details-genius-lyrics",
    "href": "2_datenbericht.html#details-genius-lyrics",
    "title": "Datenbericht",
    "section": "",
    "text": "Da wir beim Modellieren bemerkt haben, dass wir mehr Daten für bessere Modellperformance brauchen haben wir uns entschieden, die Songtexte der Tracks anzuschaffen, falls vorhanden.\nUm dies zu erreichen haben wir uns für das massive “Genius Song Lyrics” Dataset von CarlosGDCJ auf kaggle.com entschieden. Es enthält eine 9.07 GB grosse CSV Datei, welche Songtexte für über 7 Millionen Songs von der Webseite genius.com beinhaltet. Auf der einen Seite mussten wir somit nicht selber die Daten scrapen und hatten einen Grossteil der Texte für unser tracks_features.csv dataset. Auf der anderen Seite sind Spotify und Genius verschiedene Dienste, was bedeutet wir haben keinen eindeutigen Identifikator um die zwei Datasets zu joinen. Diese Problematik wird später im Kapitel Prozessierte Daten verdeutlicht.\n\n\nDer verwendete Datensatz weist keine explizite eigene Lizenz aus. Laut Beschreibung des Autors wurden die enthaltenen Daten von genius.com gescraped.\n\nFair Use: Die Nutzung erfolgt ausschliesslich zu nicht-kommerziellen, schulischen Analysezwecken; es werden keine vollständigen Songtexte veröffentlicht.\nDatenschutz: Der Datensatz wird ausschliesslich projektintern gespeichert und technisch so gesichert, dass kein unautorisierter Zugriff durch Dritte möglich ist.\nUrheberrecht: Die Texte werden nur zu analytischen Zwecken verarbeitet und nicht weiterverbreitet oder kommerziell genutzt.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndex\nName\nDatentyp\nWerte\nBeschreibung\n\n\n\n\n1\ntitle\nstring\n-\nTitel des Stücks. Meistens Songs, aber auch Bücher, Gedichte etc.\n\n\n2\ntag\nstring\n-\nGenre des Stücks. Meistens “pop”, “rap”, “rock”, “rb”, “country” oder “misc”\n\n\n3\nartist\nstring\n-\nKünstler oder Gruppe, dem das Stück zugeschrieben wird\n\n\n4\nyear\ninteger\nFormat: YYYY\nVeröffentlichungsjahr\n\n\n5\nviews\ninteger\n-\nAnzahl der Seitenaufrufe auf Genius\n\n\n6\nfeatures\nListe von strings\n-\nAlle Künstler, die beigetragen haben\n\n\n7\nlyrics\nstring\n-\nDer Songtext\n\n\n8\nid\ninteger\n-\nGenius Identifier\n\n\n9\nlanguage_cld3\nstring\nISO 639-1 Codes\nSprache des Songtextes laut CLD3\n\n\n10\nlanguage_ft\nstring\nISO 639-1 Codes\nSprache des Songtextes laut FastText\n\n\n11\nlanguage\nstring\nISO 639-1 Codes\nKombinierte Sprache (nur wenn beide Modelle übereinstimmen)\n\n\n\n(Für zusätzliche Informationen siehe Genius Song Lyrics Dataset auf Kaggle)\n\n\n\nDiese Datenanalyse wurde in Python 3.14 mit den pandas und plotly Paketen durchgeführt. Mehr Details können auf dem Repository im pyproject.toml gefunden werden. Diese Datenexploration wurde recht kurz gehalten, da wir nur die Songtexte von diesem Dataset brauchen.\n\nÜbersicht Datenqualität\n\n\nAnzahl Spalten\n11\n\n\nAnzahl Zeilen\n5134856\n\n\nAnzahl leerer Zellen\n452394\n\n\nAnteil (%) leerer Zellen\n0.8%\n\n\nAnzahl duplizierter Zeilen\n0\n\n\nAnteil (%) duplizierter Zeilen\n0%\n\n\n\n\n\nDie Spalte lyrics enthält die Songtexte. Auffällig sind hierbei die Sektions-Tags in eckigen Klammern, wie zum Beispiel [Chorus], [Verse] oder [Intro]. Diese dienen auf Genius der Strukturierung, sind jedoch kein Teil des gesungenen Textes. Um die Qualität der Textanalyse (Embeddings) zu verbessern, werden wir diese Tags im Prozessierungsschritt entfernen.\n\n\nCode\nfrom pathlib import Path\n\nimport pandas as pd\n\nGENIUS_PATH = Path(\"../data/song_lyrics.csv\")\ndf_genius = pd.read_csv(GENIUS_PATH)\n\nfirst_row = df_genius.head(1)[[\"title\", \"features\", \"lyrics\"]].copy()\nfirst_row[\"lyrics\"] = first_row[\"lyrics\"].str[:79]\ndisplay(first_row)\n\n\n\n\n\n\n\n\n\ntitle\nfeatures\nlyrics\n\n\n\n\n0\nKilla Cam\n{\"Cam\\\\'ron\",\"Opera Steve\"}\n[Chorus: Opera Steve & Cam'ron]\\nKilla Cam, Ki...\n\n\n\n\n\n\n\n\n\n\nDie artist Spalte im Genius Datensatz weist Formatierungsprobleme auf, bei denen Sonderzeichen (wie Umlaute) oft einfach weggelassen werden. Wie im folgenden Code-Beispiel ersichtlich, wird “Motörhead” zu “Motrhead” und “Blue Öyster Cult” zu “Blue yster Cult”. Interessanterweise enthält die features Spalte, welche eine Liste der beteiligten Künstler beinhaltet, oft die korrekte Schreibweise. Diese Erkenntnis ist entscheidend für das spätere Zusammenführen mit dem Spotify-Datensatz.\n\n\nCode\nfrom IPython.display import display\n\nsong_motorhead = df_genius[df_genius[\"artist\"] == \"Motrhead\"][[\"title\", \"artist\", \"features\"]]\nsong_byc = df_genius[df_genius[\"artist\"] == \"Blue yster Cult\"][[\"title\", \"artist\", \"features\"]]\ndisplay(song_motorhead.head())\ndisplay(song_byc.head())\n\n\n\n\n\n\n\n\n\ntitle\nartist\nfeatures\n\n\n\n\n50811\nAce of Spades\nMotrhead\n{Motörhead}\n\n\n54677\nOrgasmatron\nMotrhead\n{Motörhead}\n\n\n81582\nWe Are The Road Crew\nMotrhead\n{Motörhead}\n\n\n115348\nFire Fire\nMotrhead\n{Motörhead}\n\n\n121838\nIts a Long Way to the Top If You Wanna Rock N ...\nMotrhead\n{Motörhead}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntitle\nartist\nfeatures\n\n\n\n\n123021\nDont Fear The Reaper\nBlue yster Cult\n{\"Blue Öyster Cult\"}\n\n\n140918\nCareer of Evil\nBlue yster Cult\n{\"Patti Smith\",\"Blue Öyster Cult\"}\n\n\n198572\nGodzilla\nBlue yster Cult\n{\"Blue Öyster Cult\"}\n\n\n224036\nAstronomy\nBlue yster Cult\n{\"Blue Öyster Cult\"}\n\n\n234153\nBurnin’ for You\nBlue yster Cult\n{\"Blue Öyster Cult\"}\n\n\n\n\n\n\n\n\nEin weiteres Problem ist die Inkonsistenz bei der Benennung von Künstlern. Wie das Beispiel des Songs “Maneater” zeigt, werden die Künstler im Genius-Datensatz als “Hall & Oates” geführt, während sie im Spotify-Datensatz als “Daryl Hall & John Oates” auftreten. Diese Diskrepanzen erschweren einen direkten Join über den Künstlernamen und bedeuteten, dass wir einige Songs nicht matchen werden können.\n\n\nCode\ndf_genius_maneater = df_genius[(df_genius[\"title\"] == \"Maneater\") & (df_genius[\"year\"] == 1982)][\n    [\"title\", \"artist\", \"features\", \"year\"]\n]\n\nprint(\"Maneater in Genius dataset:\")\ndisplay(df_genius_maneater)\n\ndf_spotify_maneater = df[(df[\"name\"] == \"Maneater\") & (df[\"year\"] == 1982)][[\"name\", \"artists\", \"year\"]]\n\nprint(\"Maneater in Spotify dataset:\")\ndisplay(df_spotify_maneater)\n\n\nManeater in Genius dataset:\n\n\n\n\n\n\n\n\n\ntitle\nartist\nfeatures\nyear\n\n\n\n\n276117\nManeater\nHall & Oates\n{}\n1982\n\n\n\n\n\n\n\nManeater in Spotify dataset:\n\n\n\n\n\n\n\n\n\nname\nartists\nyear\n\n\n\n\n604861\nManeater\n['Daryl Hall & John Oates']\n1982\n\n\n\n\n\n\n\n\n\n\nObwohl wir primär nur an der lyrics Spalte interessiert sind, sind die Spalten title, artist und features essentiell für das Zusammenführen mit dem Spotify-Datensatz. Wie oben beschrieben, weisen genau diese Spalten Inkonsistenzen und Formatierungsprobleme auf (fehlende Sonderzeichen, abweichende Schreibweisen). Dies erforderte zusätzlichen Aufwand beim Daten-Matching, um eine möglichst hohe Trefferquote zu erzielen und sicherzustellen, dass wir so viele Songtexte wie möglich den korrekten Spotify-Tracks zuordnen können."
  },
  {
    "objectID": "2_datenbericht.html#details-finale-datasets",
    "href": "2_datenbericht.html#details-finale-datasets",
    "title": "Datenbericht",
    "section": "Details Finale Datasets",
    "text": "Details Finale Datasets\nBasierend auf den Erkenntnissen der explorativen Datenanalyse haben wir die Rohdaten in mehreren Schritten verarbeitet, um saubere und angereicherte Datensätze für die Modellierung zu erstellen.\nDer Prozess gliedert sich in drei Hauptphasen:\n\nBereinigung & Feature Engineering: Das Entfernen und Transformieren von Datenpunkten aus dem Spotify-Datensatz.\nAnreicherung: Die Verknüpfung der bereinigten Spotify-Daten mit den Genius-Songtexten.\nEmbedding Generation: Die Umwandlung der Songtexte in maschinenlesbare Vektoren mithilfe eines Embedding-Modells.\n\nDies resultierte in den oben aufgeführten Datensätzen, wobei Spotify_Cleaned_No_Noise, der verarbeitete Spotify-Datensatz, und Spotify_Genius_No_Noise_Embeddings, der Datensatz mit den Songtext Embeddings, bei der Modellierung verwendet wurden. Der Datensatz Spotify_Genius_No_Noise ist das Zwischenprodukt aus der Anreicherung des Spotify-Datensatzes mit dem Genius Datensatzes, aber vor der Generierung der Embeddings.\n\nVerworfene Zeilen\nBasierend auf den Erkenntnissen aus der Datenexploration haben wir uns entschieden, den Datensatz aggressiv zu bereinigen, um die Modellqualität zu verbessern. Folgende Filterkriterien wurden angewendet:\n\nPopularität &gt; 0: Wie in der Analyse gezeigt, haben sehr viele Tracks eine Popularität von 0. Diese “toten” Tracks sind für die Vorhersage von Hits irrelevant und verzerren das Modell.\nJahr != 0: Tracks mit dem Jahr 0 sind fehlerhafte Datenpunkte, die entfernt werden.\nSpeechiness &lt; 0.66: Da unser Fokus auf Musik liegt, entfernen wir Tracks, die hauptsächlich aus gesprochenem Wort bestehen (z.B. Hörbücher), definiert durch einen Speechiness-Wert über 0.66.\nDauer &gt; 30 Sekunden: Sehr kurze Tracks (unter 30s) sind oft Intros, Interludes oder Geräuscheffekte und keine vollwertigen Songs.\n\n\nfrom pathlib import Path\n\nimport pandas as pd\n\ndf = pd.read_csv(Path(\"../data/full_tracks_features.csv\"), keep_default_na=False, na_values=[\"\"])\n\ndf_clean = df[df[\"popularity\"] &gt; 0].copy()\ndf_clean = df_clean[df_clean[\"year\"] != 0]\ndf_clean = df_clean[df_clean[\"speechiness\"] &lt; 0.66]\ndf_clean = df_clean[df_clean[\"duration_ms\"] &gt; 30000]  # &gt; 30 seconds\n\n\n\nFeature Engineering\nUm die Daten für maschinelles Lernen nutzbar zu machen, führen wir zwei Transformationen durch:\n\nTrack Age: Das Veröffentlichungsdatum (release_date) ist in verschiedenen Formaten vorhanden und als Datum schwer direkt zu modellieren. Wir berechnen stattdessen das Alter des Tracks in Tagen (track_age_days) relativ zu einem Referenzdatum (01.01.2021). Dies gibt dem Modell eine kontinuierliche numerische Grösse für das Alter.\nExplicit Flag: Die boolesche Spalte explicit wird in eine Integer-Spalte (0/1) umgewandelt, da viele Modelle numerische Inputs bevorzugen.\n\n\ndf_clean[\"release_date\"] = pd.to_datetime(df_clean[\"release_date\"], format=\"mixed\", errors=\"coerce\")\ndf_clean[\"track_age_days\"] = (pd.Timestamp(\"2021-01-01\") - df_clean[\"release_date\"]).dt.days\ndf_clean = df_clean.drop(columns=[\"release_date\"])\n\ndf_clean[\"explicit\"] = df_clean[\"explicit\"].astype(int)\n\noutput_path = Path(\"../data/cleaned_no_noise_tracks_features.csv\")\ndf_clean.to_csv(output_path, index=False)\n\n\n\nDatasets joinen\nDa es keinen gemeinsamen Identifier zwischen Spotify und Genius gibt, erfolgt die Verknüpfung über Künstlername und Songtitel. Um die Trefferquote zu maximieren, normalisieren wir beide Felder (Kleinschreibung, Entfernen von Sonderzeichen).\nWir wenden eine zweistufige Matching-Strategie an:\n\nPrimär: Match über die artist Spalte.\nFallback: Match über die features Spalte im Genius-Datensatz, da unsere Analyse zeigte, dass dort oft korrekte Schreibweisen zu finden sind, wenn das artist Feld abweicht.\n\n\nimport ast\nimport re\n\nimport pandas as pd\n\ndf_spotify = pd.read_csv(\"../data/cleaned_no_noise_tracks_features.csv\", keep_default_na=False, na_values=[\"\"])\ndf_genius = pd.read_csv(\"../data/song_lyrics.csv\")\n\n\ndef normalize(s: str) -&gt; str:\n    return re.sub(r\"[^a-z0-9]\", \"\", str(s).lower())\n\n\ndef get_first_artist(s: str) -&gt; str:\n    try:\n        # Handle both list ['A'] and set {'A'} string representations\n        return ast.literal_eval(str(s).replace(\"{\", \"[\").replace(\"}\", \"]\"))[0]\n    except:  # noqa: E722\n        return \"\"\n\n\n# Spotify keys\ndf_spotify[\"join_artist\"] = df_spotify[\"artists\"].apply(get_first_artist).apply(normalize)\ndf_spotify[\"join_title\"] = df_spotify[\"name\"].apply(normalize)\n\n# Genius keys\ndf_genius[\"join_title\"] = df_genius[\"title\"].apply(normalize)\n\n# 1. Use 'artist' column\nlookup1 = df_genius.assign(join_artist=df_genius[\"artist\"].apply(normalize))\n# 2. Use 'features' column as fallback\nlookup2 = df_genius.assign(join_artist=df_genius[\"features\"].apply(get_first_artist).apply(normalize))\n\n# Combine lookups\nlookup = pd.concat([lookup1, lookup2])[[\"join_artist\", \"join_title\", \"lyrics\"]]\nlookup = lookup[lookup[\"join_artist\"] != \"\"].drop_duplicates(subset=[\"join_artist\", \"join_title\"])\n\n# Merge\nprint(\"Merging datasets...\")\ndf_merged = df_spotify.merge(lookup, on=[\"join_artist\", \"join_title\"], how=\"left\")\nprint(f\"Match rate: {df_merged['lyrics'].notna().mean():.1%}\")\n\ndf_merged.drop(columns=[\"join_artist\", \"join_title\"]).to_csv(\"../data/spotify_genius_merged_no_noise.csv\", index=False)\n\nMerging datasets...\nMatch rate: 34.0%\n\n\n\n\nSongtext Verarbeitung\nUm die Songtexte als Features zu nutzen, wandeln wir sie in numerische Vektoren (Embeddings) um. Wir verwenden hierfür das Modell jina-embeddings-v35.\nDabei handelt es sich um ein leistungsstarkes, mehrsprachiges Embedding-Modell mit 570 Millionen Parametern und einer Kontextlänge von 8192 Token, was es ideal für die Verarbeitung ganzer Songtexte macht. Ein Schlüsselfeature ist das sogenannte Matryoshka Representation Learning6. Dies ermöglicht es, die Dimension der Ausgabevektoren flexibel zu reduzieren, ohne die semantische Qualität signifikant zu beeinträchtigen. Wir nutzen dies, um die Embeddings von den ursprünglichen 1024 auf kompakte 128 Dimensionen zu kürzen. Dies spart massiv Speicherplatz und beschleunigt das Training der nachfolgenden Regressionsmodelle.\nAufgrund der Grösse des Datensatzes war dieser Schritt sehr rechenintensiv und wurde auf einer Azure Compute Node mit einer NVIDIA A100 GPU durchgeführt.\n\nfrom pathlib import Path\n\nimport pandas as pd\nimport torch\nfrom sentence_transformers import SentenceTransformer\n\nDATASET = Path(\"../data/spotify_genius_merged_no_noise.csv\")\nTARGET_DIM = 128\n\nmodel = SentenceTransformer(\n    \"jinaai/jina-embeddings-v3\", trust_remote_code=True, model_kwargs={\"torch_dtype\": torch.float16}\n)\nmodel.to(\"cuda\")\n\ndf = pd.read_csv(DATASET, keep_default_na=False, na_values=[\"\"])\n\n\ndf[\"lyrics\"] = df[\"lyrics\"].fillna(\"\")\ndf[\"lyrics\"] = df[\"lyrics\"].astype(str)\n\nprint(\"Starting encoding...\")\nembeddings = model.encode(\n    df[\"lyrics\"].tolist(), task=\"text-matching\", truncate_dim=TARGET_DIM, batch_size=32, show_progress_bar=True\n)\n\ndf[\"embedding\"] = list(embeddings)\n\noutput_path = Path(\"data/spotify_genius_embeddings_V3.parquet\")\ndf.to_parquet(output_path, index=False)\n\n\n\nDatenkatalog\n\n\n\n\n\n\n\n\n\n\nIndex\nName\nDatentyp\nWerte\nBeschreibung\n\n\n\n\n1\nid\nstring\nFormat: base-62\nDie Spotify-ID für den Track\n\n\n2\nname\nstring\n-\nDer Name des Tracks\n\n\n3\nalbum\nstring\n-\nDas Album, auf dem der Track erscheint\n\n\n4\nalbum_id\nstring\nFormat: base-62\nDie Spotify-ID für das Album\n\n\n5\nartists\nListe von strings\n-\nDie Künstler, die den Track performt haben\n\n\n6\nartist_id\nListe von strings\nFormat: base-62\nDie Spotify-ID für die Künstler\n\n\n7\ntrack_number\ninteger\nWertebereich: 1 - 50\nDie Nummer des Tracks auf dem Album\n\n\n8\ndisc_number\ninteger\nWertebereich: 1 - 13\nDie Disc-Nummer, auf dem der Track erscheint\n\n\n9\nexplicit\ninteger\n1 = Ja0 = Nein oder unbekannt\nOb der Track explizite Texte enthält\n\n\n10\ndanceability\nfloat\nWertebereich: 0 - 1\nTanzbarkeit beschreibt, wie geeignet ein Track zum Tanzen ist, basierend auf einer Kombination musikalischer Elemente.0.0 → am wenigsten tanzbar, 1.0 → am tanzbarsten\n\n\n11\nenergy\nfloat\nWertebereich: 0 - 1\nWahrnehmungsmass für Intensität und Aktivität dar, typischerweise fühlen sich energiegeladene Tracks schnell, laut und geräuschvoll an\n\n\n12\nkey\ninteger\nWertebereich: -1 - 11\nDie Tonart, in der sich der Track befindet, basierend auf Standard-Pitch-Class-Notation, Wert -1 = keine Tonart erkannt\n\n\n13\nloudness\nfloat\nWertebereich: -60 - 0Einheit: Dezibel (dB)\nDie Gesamtlautstärke eines Tracks in Dezibel (dB)\n\n\n14\nmode\ninteger\n1 = Major, 0 = Minor\nGibt die Tonalität (Dur oder Moll) eines Tracks an\n\n\n15\nspeechiness\nfloat\nWertebereich: 0 - 1\nErkennt das Vorhandensein von gesprochenen Worten in einem Track\n\n\n16\nacousticness\nfloat\nWertebereich: 0 - 1\nKonfidenzmass ob der Track akustisch ist\n\n\n17\ninstrumentalness\nfloat\nWertebereich: 0 - 1\nSagt voraus, ob ein Track keinen Gesang enthält\n\n\n18\nliveness\nfloat\nWertebereich: 0 - 1\nErkennt die Anwesenheit eines Publikums in der Aufnahme\n\n\n19\nvalence\nfloat\nWertebereich: 0 - 1\nBeschreibt die musikalische Positivität, die von einem Track vermittelt wird\n\n\n20\ntempo\nfloat\nEinheit: beats per minute (BPM)\nDas geschätzte Gesamttempo eines Tracks in Schlägen pro Minute (BPM)\n\n\n21\nduration_ms\nfloat\nEinheit: Millisekunden (ms)\nDie Dauer des Tracks in Millisekunden.\n\n\n22\ntime_signature\ninteger\nWertebereich: 3 - 7\nEine geschätzte Taktart, gibt wie viele Schläge in jedem Takt enthalten sind\n\n\n23\nyear\ninteger\nFormat: YYYY\nDas Jahr des Veröffentlichungsdatums des Tracks\n\n\n24\npopularity\ninteger\nWertebereich: 0 - 100\nDie Popularität des Tracks, basiert hauptsächlich auf Gesamtzahl der Wiedergaben\n\n\n25\nlyrics\nstring\n-\nDer Songtext ohne Sektiontags\n\n\n26\nembedding\nListe von floats\nDimension: 128\nSemantische Vektor-Repräsentation des Songtextes (Jina V3)\n\n\n27\ntrack_age_days\ninteger\nEinheit: Tage\nAlter des Tracks in Tagen relativ zum 01.01.2021\n\n\n\n\n\nDatenqualität\nDiese Datenanalyse wurde in Python 3.14 mit den pandas und plotly Paketen durchgeführt. Mehr Details können auf dem Repository im pyproject.toml gefunden werden.\n\nÜbersicht Datenqualität\n\n\nAnzahl Spalten\n27\n\n\nAnzahl Zeilen\n471’478\n\n\nAnzahl leerer Zellen\n0\n\n\nAnteil (%) leerer Zellen\n0%\n\n\nAnzahl duplizierter Zeilen\n0\n\n\nAnteil (%) duplizierter Zeilen\n0%\n\n\n\nDataset Reduktion\nDurch die Anwendung der oben definierten Filter (Popularität &gt; 0, Speechiness &lt; 0.66, Dauer &gt; 30s, etc.) reduzierte sich die Anzahl der Beobachtungen von ursprünglich über 1.2 Millionen auf 471’478. Dies entspricht einer Reduktion von rund 61%. Wir betrachten diesen Verlust als notwendige Qualitätssteigerung, da wir so Rauschen (“Noise”) aus dem Datensatz entfernen und uns auf relevante Musikstücke konzentrieren.\nSongtexte\nNach dem Merge-Prozess konnten für 34% der Tracks im bereinigten Datensatz Songtexte gefunden werden. Wir haben diverse Stichproben durchgeführt und sind mit der Qualität der Zuordnung zufrieden. Tracks ohne gefundene Lyrics erhalten einen leeren String als Wert, was vom Embedding-Modell entsprechend verarbeitet wird.\n\nVerteilung popularity nach Bereinigung\nDurch das Entfernen der inaktiven Tracks (Popularität = 0) hat sich die Verteilung der Zielvariable stark verändert. Während im Rohdatensatz die Null-Werte dominierten, sehen wir nun eine Verteilung, die sich eher für Regressionsaufgaben eignet. Es ist zwar immer noch eine Rechtsschiefe erkennbar (es gibt weniger Super-Hits als moderate Songs), aber die “Wand” bei 0 ist verschwunden.\n\n\nCode\nimport pandas as pd\nimport plotly.express as px\nimport plotly.io as pio\n\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\n\n\ndf_final = pd.read_csv(\"../data/spotify_genius_merged_no_noise.csv\", keep_default_na=False, na_values=[\"\"])\n\nfig = px.histogram(\n    df_final,\n    x=\"popularity\",\n    log_y=True,\n    title=\"Verteilung der Popularität (Bereinigter Datensatz)\",\n    labels={\"popularity\": \"Popularity\"},\n    nbins=100,\n)\n\nfig.update_layout(yaxis_title=\"Anzahl Tracks\", xaxis_title=\"Popularity (1-100)\", showlegend=False, bargap=0.1)\n\nfig.show()\n\n\n                            \n                                            \n\n\n\n\nAnalyse der Lyrics-Abdeckung\nDa wir nur für ca. 34% der Tracks Songtexte finden konnten, prüfen wir hier auf systematischen Bias.\nDie Grafik zeigt deutlich: Tracks mit Lyrics sind im Durchschnitt fast doppelt so populär (17.8 vs 9.4). Dies ist ein erwarteter und positiver Bias, da auf der Community-Plattform Genius primär Texte für relevante, populäre Songs gepflegt werden. Somit ist die Datenqualität genau dort am höchsten, wo es für unsere Vorhersage am wichtigsten ist.\n\n\nCode\ndf_final[\"has_lyrics\"] = df_final[\"lyrics\"].str.len() &gt; 0\ndf_final[\"status\"] = df_final[\"has_lyrics\"].map({True: \"Mit Lyrics\", False: \"Ohne Lyrics\"})\n\nfig = px.box(\n    df_final,\n    x=\"status\",\n    y=\"popularity\",\n    color=\"status\",\n    title=\"Popularitäts-Verteilung: Mit vs. Ohne Lyrics\",\n    points=False,\n)\n\nfig.update_layout(xaxis_title=\"Status\", yaxis_title=\"Popularity\", showlegend=False)\n\nfig.show()\n\n# Calculate mean popularity\nmean_pop_with = df_final[df_final[\"has_lyrics\"]][\"popularity\"].mean()\nmean_pop_without = df_final[~df_final[\"has_lyrics\"]][\"popularity\"].mean()\n\nprint(f\"Durchschnittliche Popularität mit Lyrics: {mean_pop_with:.2f}\")\nprint(f\"Durchschnittliche Popularität ohne Lyrics: {mean_pop_without:.2f}\")\n\n\n                            \n                                            \n\n\nDurchschnittliche Popularität mit Lyrics: 17.83\nDurchschnittliche Popularität ohne Lyrics: 9.35\n\n\n\n\nEinschätzung Datenqualität\nZusammenfassend verfügen wir nun über einen hochwertigen, bereinigten Datensatz, der spezifisch auf die Vorhersage von Musik-Popularität zugeschnitten ist. Durch die aggressive Filterung von Rauschen (inaktive Tracks, Soundeffekte) und die Anreicherung mit semantischen Songtext-Embeddings haben wir die Informationsdichte für relevante Tracks maximiert ohne externe Faktoren zu den Liedern herzunehmen."
  },
  {
    "objectID": "2_datenbericht.html#footnotes",
    "href": "2_datenbericht.html#footnotes",
    "title": "Datenbericht",
    "section": "Fußnoten",
    "text": "Fußnoten\n\n\nhttps://developer.spotify.com/documentation/web-api/concepts/spotify-uris-ids↩︎\nhttps://developer.spotify.com/documentation/web-api/reference/get-audio-features#:~:text=speechiness↩︎\nhttps://en.wikipedia.org/wiki/Arabian_Waltz↩︎\nhttps://en.wikipedia.org/wiki/Loudness_war↩︎\nhttps://jina.ai/models/jina-embeddings-v3/↩︎\nhttps://huggingface.co/blog/matryoshka#%F0%9F%AA%86-matryoshka-embeddings↩︎"
  }
]